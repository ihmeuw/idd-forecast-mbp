{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"idd-forecast-mbp","text":"<p>Documentation: https://ihmeuw.github.io/idd-forecast-mbp</p> <p>Source Code: https://github.com/ihmeuw/idd-forecast-mbp</p> <p>Word</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install idd-forecast-mbp\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.10+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the <code>docs</code> directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github project page   automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatting (<code>ruff format</code>), linters (e.g. <code>ruff</code> and <code>mypy</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"test1/","title":"Test1","text":"<p>Text in here</p>"},{"location":"test3/","title":"Test3","text":"<p>Other text here</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>idd_forecast_mbp<ul> <li>cli</li> <li>constants</li> <li>data</li> <li>helper_functions</li> <li>map_to_admin_2<ul> <li>01_prep_maps</li> <li>02_pixel_main_parallel</li> <li>03_pixel_hierarchy_parallel</li> <li>pixel_hierarchy</li> <li>pixel_main</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/idd_forecast_mbp/","title":"idd_forecast_mbp","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data","title":"<code>data</code>","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData","title":"<code>FloodingData</code>","text":"<p>Class to handle flooding data.</p> Source code in <code>src/idd_forecast_mbp/data.py</code> <pre><code>class FloodingData:\n    \"\"\"\n    Class to handle flooding data.\n    \"\"\"\n\n    def __init__(self, root: str | Path = rfc.MODEL_ROOT):\n        self._root = Path(root)\n\n    @property\n    def root(self) -&gt; Path:\n        \"\"\"\n        Returns the root path of the flooding data.\n        \"\"\"\n        return self._root\n\n    @property\n    def logs(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the logs directory.\n        \"\"\"\n        return self.root / \"logs\"\n\n    def log_dir(self, step_name: str) -&gt; Path:\n        return self.logs / step_name\n\n    @property\n    def raw_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"01-raw_data\"\n\n    @property\n    def processed_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"02-processed_data\"\n\n    @property\n    def modeling_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"03-modeling_data\"\n\n    @property\n    def forecasting_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"04-forecasting_data\"\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.forecasting_root","title":"<code>forecasting_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.logs","title":"<code>logs: Path</code>  <code>property</code>","text":"<p>Returns the path to the logs directory.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.modeling_root","title":"<code>modeling_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.processed_root","title":"<code>processed_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.raw_root","title":"<code>raw_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.data.FloodingData.root","title":"<code>root: Path</code>  <code>property</code>","text":"<p>Returns the root path of the flooding data.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2","title":"<code>map_to_admin_2</code>","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps","title":"<code>01_prep_maps</code>","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates","title":"<code>batch_process_all_covariates(output_dir=None, skip_existing=True)</code>","text":"<p>Process all covariates in the COVARIATE_DICT and convert them to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--parameters","title":"Parameters:","text":"<p>output_dir : str or Path, optional     Directory to save output files. If None, uses default directory. skip_existing : bool, default=True     If True, skip processing covariates that already have output files.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--returns","title":"Returns:","text":"<p>dict: Dictionary with covariate names as keys and output paths as values</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def batch_process_all_covariates(output_dir=None, skip_existing=True):\n    \"\"\"\n    Process all covariates in the COVARIATE_DICT and convert them to netCDF.\n\n    Parameters:\n    -----------\n    output_dir : str or Path, optional\n        Directory to save output files. If None, uses default directory.\n    skip_existing : bool, default=True\n        If True, skip processing covariates that already have output files.\n\n    Returns:\n    --------\n    dict: Dictionary with covariate names as keys and output paths as values\n    \"\"\"\n    if output_dir is None:\n        output_dir = OUTPUT_PATH\n    else:\n        output_dir = Path(output_dir)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    results = {}\n\n    for covariate_key in COVARIATE_DICT.keys():\n        print(f\"\\nProcessing {covariate_key}...\")\n        covariate_dict = parse_yaml_dictionary(covariate_key)\n        covariate_name = covariate_dict['covariate_name']\n\n        output_path = output_dir / f\"{covariate_name}.nc\"\n\n        # Skip if output file already exists and skip_existing is True\n        if skip_existing and os.path.exists(output_path):\n            print(f\"Output file {output_path} already exists. Skipping.\")\n            results[covariate_name] = str(output_path)\n            continue\n\n        try:\n            synoptic = covariate_dict['synoptic']\n            years = covariate_dict['years']\n\n            if synoptic or len(years) == 1:\n                # Process as synoptic variable\n                results[covariate_name] = process_synoptic_variable(covariate_key, output_dir)\n            else:\n                # Process as multi-year variable\n                results[covariate_name] = process_multiyear_variable(covariate_key, output_dir)\n        except Exception as e:\n            print(f\"Error processing {covariate_name}: {str(e)}\")\n            results[covariate_name] = None\n\n    # Display summary\n    print(\"\\n==== Processing Summary ====\")\n    success_count = sum(1 for path in results.values() if path is not None)\n    print(f\"Successfully processed {success_count} out of {len(results)} covariates\")\n\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf","title":"<code>create_multiyear_netcdf(input_path_template, years, output_path, variable_name)</code>","text":"<p>Create a multi-year netCDF file from individual GeoTIFF files.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--parameters","title":"Parameters:","text":"<p>input_path_template : str     Template path with {year} to be replaced for each year years : list of int     List of years to process output_path : str     Path to save the combined netCDF file variable_name : str     Name of the variable in the output file</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset with time dimension</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def create_multiyear_netcdf(input_path_template, years, output_path, variable_name):\n    \"\"\"\n    Create a multi-year netCDF file from individual GeoTIFF files.\n\n    Parameters:\n    -----------\n    input_path_template : str\n        Template path with {year} to be replaced for each year\n    years : list of int\n        List of years to process\n    output_path : str\n        Path to save the combined netCDF file\n    variable_name : str\n        Name of the variable in the output file\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset with time dimension\n    \"\"\"\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = input_path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(input_path, temp_path, variable_name, year=year)\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    # Sort by time\n    if combined_ds is not None:\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Update time attributes to ensure it's treated as simple year integers\n        if combined_ds is not None:\n            combined_ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\", \n                \"units\": \"year\"\n            }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        return combined_ds\n    else:\n        print(\"No valid data files found.\")\n        return None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf","title":"<code>geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None, global_extent=True, fill_value=0.0)</code>","text":"<p>Convert a GeoTIFF file to netCDF format.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--parameters","title":"Parameters:","text":"<p>input_path : str     Path to the input GeoTIFF file output_path : str     Path to save the output netCDF file variable_name : str     Name of the variable to use in the netCDF file resolution : float, optional     Resolution of the raster in degrees. If None, will be calculated from the transform. year : int, optional     Year for the data (for time dimension) encoding : dict, optional     Encoding options for xarray.to_netcdf() global_extent : bool, default=False     If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90) fill_value : float, default=0.0     Value to fill extended areas with</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None,\n                      global_extent=True, fill_value=0.0):\n    \"\"\"\n    Convert a GeoTIFF file to netCDF format.\n\n    Parameters:\n    -----------\n    input_path : str\n        Path to the input GeoTIFF file\n    output_path : str\n        Path to save the output netCDF file\n    variable_name : str\n        Name of the variable to use in the netCDF file\n    resolution : float, optional\n        Resolution of the raster in degrees. If None, will be calculated from the transform.\n    year : int, optional\n        Year for the data (for time dimension)\n    encoding : dict, optional\n        Encoding options for xarray.to_netcdf()\n    global_extent : bool, default=False\n        If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90)\n    fill_value : float, default=0.0\n        Value to fill extended areas with\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Open the GeoTIFF file\n    with rio.open(input_path) as src:\n        # Read the data and transform information\n        data = src.read(1)  # Read first band\n        height, width = data.shape\n\n        # Handle no-data values\n        if src.nodata is not None:\n            data = np.where(data == src.nodata, np.nan, data)\n\n        # Get geospatial information\n        transform = src.transform\n\n        # Use provided resolution or calculate it from transform\n        if resolution is None:\n            x_res = transform[0]  # Width of a pixel\n            y_res = abs(transform[4])  # Height of a pixel\n        else:\n            x_res = y_res = resolution  # Use the square pixels resolution from YAML\n\n        # Calculate bounds correctly with full pixel coverage\n        xmin = transform[2]  # Left edge\n        ymax = transform[5]  # Top edge\n\n        # Use resolution to calculate exact right and bottom edges\n        xmax = xmin + (width * x_res)  # Right edge (including full pixel width)\n        ymin = ymax - (height * y_res)  # Bottom edge (including full pixel height)\n\n        if global_extent:\n            # Define global extents\n            global_xmin, global_xmax = -180.0, 180.0\n            global_ymin, global_ymax = -90.0, 90.0\n\n            # Calculate number of pixels needed for global coverage\n            global_width = int(np.ceil((global_xmax - global_xmin) / x_res))\n            global_height = int(np.ceil((global_ymax - global_ymin) / y_res))\n\n            # Create empty global array filled with fill_value\n            global_data = np.full((global_height, global_width), fill_value, dtype=data.dtype)\n\n            # Calculate indices to place original data\n            x_start = int(round((xmin - global_xmin) / x_res))\n            y_start = int(round((global_ymax - ymax) / y_res))\n\n            # Ensure indices are within bounds\n            x_start = max(0, x_start)\n            y_start = max(0, y_start)\n\n            # Place the original data into the global grid\n            x_end = min(x_start + width, global_width)\n            y_end = min(y_start + height, global_height)\n\n            # Calculate how much of the original data we can fit\n            orig_x_slice = slice(0, x_end - x_start)\n            orig_y_slice = slice(0, y_end - y_start)\n\n            # Place the data\n            global_data[y_start:y_end, x_start:x_end] = data[orig_y_slice, orig_x_slice]\n\n            # Use this data for the rest of the function\n            data = global_data\n            xmin, xmax = global_xmin, global_xmax\n            ymin, ymax = global_ymin, global_ymax\n            width, height = global_width, global_height\n\n        # Create coordinate arrays for pixel centers (not edges)\n        lons = np.linspace(xmin + x_res/2, xmax - x_res/2, width)\n        lats = np.linspace(ymax - y_res/2, ymin + y_res/2, height)\n\n        # Create xarray dataset\n        if year is not None:\n            # Create a dataset with time dimension using integer year\n            time_val = np.array([int(year)])  # Ensure it's an integer in an array\n            ds = xr.Dataset(\n                {\"value\": ([\"time\", \"lat\", \"lon\"], data[np.newaxis, :, :])},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"time\": time_val\n                }\n            )\n\n            # Add time coordinate attributes indicating it's just a year, not a timestamp\n            ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\",\n                \"units\": \"year\"\n            }\n        else:\n            # Create a dataset without time dimension (synoptic)\n            ds = xr.Dataset(\n                {\"value\": ([\"lat\", \"lon\"], data)},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats\n                }\n            )\n\n        # Add variable attributes\n        ds[\"value\"].attrs = {\n            \"long_name\": variable_name.replace(\"_\", \" \"),\n            \"units\": src.meta.get(\"units\", \"unknown\"),\n            \"resolution\": x_res  # Add resolution to metadata\n        }\n\n        # Add coordinate attributes\n        ds.lon.attrs = {\n            \"long_name\": \"longitude\",\n            \"standard_name\": \"longitude\",\n            \"units\": \"degrees_east\",\n            \"axis\": \"X\",\n            \"resolution\": x_res\n        }\n\n        ds.lat.attrs = {\n            \"long_name\": \"latitude\",\n            \"standard_name\": \"latitude\",\n            \"units\": \"degrees_north\",\n            \"axis\": \"Y\",\n            \"resolution\": y_res\n        }\n\n        # Add global attributes\n        ds.attrs = {\n            \"title\": f\"{variable_name} data\",\n            \"source\": f\"Converted from GeoTIFF: {os.path.basename(input_path)}\",\n            \"created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"Conventions\": \"CF-1.8\",\n            \"history\": f\"Created from {input_path}\",\n            \"pixel_size_degrees\": x_res,\n            \"pixel_registration\": \"center\"\n        }\n\n        # Use default encoding if none provided\n        if encoding is None:\n            encoding = {\n                \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n                \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n                \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5}\n            }\n            if year is not None:\n                # Ensure time is stored as a simple integer with no date units\n                encoding[\"time\"] = {\"dtype\": \"int32\"}\n\n        # Save the dataset to netCDF\n        ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n        return ds\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.main","title":"<code>main()</code>","text":"<p>Main function to run when script is executed directly.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def main():\n    \"\"\"Main function to run when script is executed directly.\"\"\"\n    results = batch_process_all_covariates(skip_existing=False)\n\n    # Print successful conversions\n    print(\"\\nSuccessfully converted covariates:\")\n    for covariate, path in results.items():\n        if path:\n            print(f\"- {covariate}: {path}\")\n\n    # Print failed conversions\n    print(\"\\nFailed conversions:\")\n    for covariate, path in results.items():\n        if not path:\n            print(f\"- {covariate}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable","title":"<code>process_multiyear_variable(covariate_key, output_dir)</code>","text":"<p>Process a multi-year variable and convert to a single netCDF with time dimension.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_multiyear_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a multi-year variable and convert to a single netCDF with time dimension.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path_template = covariate_dict['path']\n    years = covariate_dict['years']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    if not years:\n        print(f\"No years specified for {covariate_name}. Cannot process as multi-year variable.\")\n        return None\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Update create_multiyear_netcdf to pass resolution to geotiff_to_netcdf\n    # This requires modifying the create_multiyear_netcdf function\n\n    # For now, pass resolution directly to each geotiff_to_netcdf call\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(\n            input_path=input_path, \n            output_path=temp_path, \n            variable_name=covariate_name, \n            resolution=resolution,\n            year=year\n        )\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    if combined_ds is not None:\n        # Sort by time\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n    return str(output_path) if combined_ds is not None else None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable","title":"<code>process_synoptic_variable(covariate_key, output_dir)</code>","text":"<p>Process a synoptic (time-invariant) variable and convert to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_synoptic_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a synoptic (time-invariant) variable and convert to netCDF.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path = covariate_dict['path']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Convert to netCDF with resolution from YAML\n    geotiff_to_netcdf(\n        input_path=path,\n        output_path=str(output_path),\n        variable_name=covariate_name,\n        resolution=resolution\n    )\n\n    return str(output_path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.set_file_permissions","title":"<code>set_file_permissions(file_path)</code>","text":"<p>Set 775 permissions (rwxrwxr-x) on the specified file.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def set_file_permissions(file_path):\n    \"\"\"Set 775 permissions (rwxrwxr-x) on the specified file.\"\"\"\n    try:\n        os.chmod(file_path, 0o775)  # 0o775 is octal for 775 permissions\n    except Exception as e:\n        print(f\"Warning: Could not set permissions on {file_path}: {str(e)}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy","title":"<code>pixel_hierarchy</code>","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy","title":"<code>aggregate_climate_to_hierarchy(data: pd.DataFrame, hierarchy: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Create all aggregate climate values for a given hierarchy from most-detailed data.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--parameters","title":"Parameters","text":"<p>data     The most-detailed climate data to aggregate. hierarchy     The hierarchy to aggregate the data to.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The climate data with values for all levels of the hierarchy.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def aggregate_climate_to_hierarchy(\n    data: pd.DataFrame, hierarchy: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Create all aggregate climate values for a given hierarchy from most-detailed data.\n\n    Parameters\n    ----------\n    data\n        The most-detailed climate data to aggregate.\n    hierarchy\n        The hierarchy to aggregate the data to.\n\n    Returns\n    -------\n    pd.DataFrame\n        The climate data with values for all levels of the hierarchy.\n    \"\"\"\n    results = data.set_index(\"location_id\").copy()\n\n    # Most detailed locations can be at multiple levels of the hierarchy,\n    # so we loop over all levels from most detailed to global, aggregating\n    # level by level and appending the results to the data.\n    for level in reversed(list(range(1, hierarchy.level.max() + 1))):\n        level_mask = hierarchy.level == level\n        parent_map = hierarchy.loc[level_mask].set_index(\"location_id\").parent_id\n\n        # For every location in the parent map, we need to check if it is the results\n        # For those that are, proceed to aggregate\n        # For those that aren't, check to make sure their parent is in the results. If not, exit with an error\n        absent_parent_map = parent_map.index.difference(results.index)\n        if len(absent_parent_map) &gt; 0:\n            msg = f\"Some parent locations are not in the results: {absent_parent_map}\"\n            # Check to see if the parent of each location id that is missing is in the results\n            parent_of_absent = parent_map.loc[absent_parent_map]\n            unique_parent_ids = parent_of_absent.unique()\n            # Check to see if the unique_parent_ids are in the results\n            missing_parents = unique_parent_ids[~np.isin(unique_parent_ids, results.index)]\n            if len(missing_parents) &gt; 0:\n                msg = f\"Some parent locations are not in the results: {missing_parents}\"\n                raise ValueError(msg)\n\n        present_parent_map = parent_map.loc[parent_map.index.isin(results.index)]\n        # Continue aggregation only on the present locations\n        subset = results.loc[present_parent_map.index]\n        subset[\"parent_id\"] = present_parent_map\n\n        parent_values = (\n            subset.groupby([\"year_id\", \"parent_id\"])[[\"weighted_climate\", \"population\"]]\n            .sum()\n            .reset_index()\n            .rename(columns={\"parent_id\": \"location_id\"})\n            .set_index(\"location_id\")\n        )\n        results = pd.concat([results, parent_values])\n    results = (\n        results.reset_index()\n        .sort_values([\"location_id\", \"year_id\"])\n    )\n    parent_values[\"value\"] = parent_values.weighted_climate / parent_values.population\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy","title":"<code>load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame</code>","text":"<p>Load a subset location hierarchy.</p> <p>The subset hierarchy might be equal to the full aggregation hierarchy, but it might also be a subset of the full aggregation hierarchy. These hierarchies are used to provide different views of aggregated climate data.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--parameters","title":"Parameters","text":"<p>subset_hierarchy     The administrative hierarchy to load (e.g. \"gbd_2021\")</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The hierarchy data with parent-child relationships</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame:\n    \"\"\"Load a subset location hierarchy.\n\n    The subset hierarchy might be equal to the full aggregation hierarchy,\n    but it might also be a subset of the full aggregation hierarchy.\n    These hierarchies are used to provide different views of aggregated\n    climate data.\n\n    Parameters\n    ----------\n    subset_hierarchy\n        The administrative hierarchy to load (e.g. \"gbd_2021\")\n\n    Returns\n    -------\n    pd.DataFrame\n        The hierarchy data with parent-child relationships\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    allowed_hierarchies = [\"gbd_2021\", \"fhs_2021\", \"lsae_1209\", \"lsae_1285\"]\n    if subset_hierarchy not in allowed_hierarchies:\n        msg = f\"Unknown admin hierarchy: {subset_hierarchy}\"\n        raise ValueError(msg)\n    path = root / \"gbd-inputs\" / f\"hierarchy_{subset_hierarchy}.parquet\"\n    return pd.read_parquet(path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.post_process","title":"<code>post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Rename 000 to {summary_covariate}_per_capita Merge in population Create {summary_covariate}_capita*population -&gt; {summary_covariate}</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame: # Fix this for other summary_variable/variable/etc\n    \"\"\"\n    Rename 000 to {summary_covariate}_per_capita\n    Merge in population\n    Create {summary_covariate}_capita*population -&gt; {summary_covariate}\n    \"\"\"\n\n    # Rename 000 to people_flood_days_per_capita\n    df = df.rename(columns={\"000\": f\"{summary_covariate}_per_capita\"})\n\n    # Merge in population\n    full_df = df.merge(\n        pop_df,\n        on=[\"location_id\", \"year_id\"],\n        how=\"left\",\n    )\n    # assert all location_ids and years combinations are present\n    assert df.shape[0] == full_df.shape[0]\n    assert df.location_id.nunique() == full_df.location_id.nunique()\n    assert df.year_id.nunique() == full_df.year_id.nunique()\n\n    # Create {summary_covariate}\n    full_df[summary_covariate] = (\n        full_df[f\"{summary_covariate}_per_capita\"] * full_df[\"population\"]\n    ).astype(np.float32)\n\n    return full_df\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main","title":"<code>pixel_main</code>","text":""},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map","title":"<code>build_bounds_map(raster_template: rt.RasterArray, shape_values: list[tuple[Polygon | MultiPolygon, int]]) -&gt; dict[int, tuple[slice, slice]]</code>","text":"<p>Build a map of location IDs to buffered slices of the raster template.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--parameters","title":"Parameters","text":"<p>raster_template     The raster template to build the bounds map for. shape_values     A list of tuples where the first element is a shapely Polygon or MultiPolygon     in the CRS of the raster template and the second element is the location ID     of the shape.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--returns","title":"Returns","text":"<p>dict[int, tuple[slice, slice]]     A dictionary mapping location IDs to a tuple of slices representing the bounds     of the location in the raster template. The slices are buffered by 10 pixels     to ensure that the entire shape is included in the mask.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def build_bounds_map(\n    raster_template: rt.RasterArray,\n    shape_values: list[tuple[Polygon | MultiPolygon, int]],\n) -&gt; dict[int, tuple[slice, slice]]:\n    \"\"\"Build a map of location IDs to buffered slices of the raster template.\n\n    Parameters\n    ----------\n    raster_template\n        The raster template to build the bounds map for.\n    shape_values\n        A list of tuples where the first element is a shapely Polygon or MultiPolygon\n        in the CRS of the raster template and the second element is the location ID\n        of the shape.\n\n    Returns\n    -------\n    dict[int, tuple[slice, slice]]\n        A dictionary mapping location IDs to a tuple of slices representing the bounds\n        of the location in the raster template. The slices are buffered by 10 pixels\n        to ensure that the entire shape is included in the mask.\n    \"\"\"\n    # The tranform maps pixel coordinates to the CRS coordinates.\n    # This mask is the inverse of that transform.\n    to_pixel = ~raster_template.transform\n\n    bounds_map = {}\n    for shp, loc_id in shape_values:\n        xmin, ymin, xmax, ymax = shp.bounds\n        pxmin, pymin = to_pixel * (xmin, ymax)\n        pixel_buffer = 10\n        pxmin = max(0, int(pxmin) - pixel_buffer)\n        pymin = max(0, int(pymin) - pixel_buffer)\n        pxmax, pymax = to_pixel * (xmax, ymin)\n        pxmax = min(raster_template.width, int(pxmax) + pixel_buffer)\n        pymax = min(raster_template.height, int(pymax) + pixel_buffer)\n        bounds_map[loc_id] = (slice(pymin, pymax), slice(pxmin, pxmax))\n\n    return bounds_map\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox","title":"<code>get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon</code>","text":"<p>Get the bounding box of a raster array.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--parameters","title":"Parameters","text":"<p>raster     The raster array to get the bounding box of. crs     The CRS to return the bounding box in. If None, the bounding box     is returned in the CRS of the raster.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--returns","title":"Returns","text":"<p>shapely.Polybon     The bounding box of the raster in the CRS specified by the crs parameter.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon:\n    \"\"\"Get the bounding box of a raster array.\n\n    Parameters\n    ----------\n    raster\n        The raster array to get the bounding box of.\n    crs\n        The CRS to return the bounding box in. If None, the bounding box\n        is returned in the CRS of the raster.\n\n    Returns\n    -------\n    shapely.Polybon\n        The bounding box of the raster in the CRS specified by the crs parameter.\n    \"\"\"\n    if raster.crs not in MAX_BOUNDS:\n        msg = f\"Unsupported CRS: {raster.crs}\"\n        raise ValueError(msg)\n\n    xmin_clip, xmax_clip, ymin_clip, ymax_clip = MAX_BOUNDS[raster.crs]\n    xmin, xmax, ymin, ymax = raster.bounds\n\n    xmin = np.clip(xmin, xmin_clip, xmax_clip)\n    xmax = np.clip(xmax, xmin_clip, xmax_clip)\n    ymin = np.clip(ymin, ymin_clip, ymax_clip)\n    ymax = np.clip(ymax, ymin_clip, ymax_clip)\n\n    bbox = gpd.GeoSeries([shapely.box(xmin, ymin, xmax, ymax)], crs=raster.crs)\n    out_bbox = bbox.to_crs(crs) if crs is not None else bbox.copy()\n\n    # Check that our transformation didn't do something weird\n    # (e.g. artificially clip the bounds or have the bounds extend over the\n    # antimeridian)\n    check_bbox = out_bbox.to_crs(raster.crs)\n    area_change = (np.abs(bbox.area - check_bbox.area) / bbox.area).iloc[0]\n    tolerance = 1e-6\n    if area_change &gt; tolerance:\n        msg = f\"Area change: {area_change}\"\n        raise ValueError(msg)\n\n    return cast(shapely.Polygon, out_bbox.iloc[0])\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes","title":"<code>load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]) -&gt; gpd.GeoDataFrame</code>","text":"<p>Load shapes for a full aggregation hierarchy within given bounds.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--parameters","title":"Parameters","text":"<p>full_aggregation_hierarchy     The full aggregation hierarchy to load (e.g. \"gbd_2021\") bounds     The bounds to load (xmin, ymin, xmax, ymax)</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--returns","title":"Returns","text":"<p>gpd.GeoDataFrame     The shapes for the given hierarchy and bounds</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load shapes for a full aggregation hierarchy within given bounds.\n\n    Parameters\n    ----------\n    full_aggregation_hierarchy\n        The full aggregation hierarchy to load (e.g. \"gbd_2021\")\n    bounds\n        The bounds to load (xmin, ymin, xmax, ymax)\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        The shapes for the given hierarchy and bounds\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    if full_aggregation_hierarchy in [\"gbd_2021\", \"gbd_2023\"]:\n        shape_path = (\n            root/ f\"shapes_{full_aggregation_hierarchy}.parquet\"\n        )\n        gdf = gpd.read_parquet(shape_path, bbox=bounds)\n\n        # We're using population data here instead of a hierarchy because\n        # The populations include extra locations we've supplemented that aren't\n        # modeled in GBD (e.g. locations with zero population or places that\n        # GBD uses population scalars from WPP to model)\n        pop_path = (\n            root / f\"population_{full_aggregation_hierarchy}.parquet\"\n        )\n        pop = pd.read_parquet(pop_path)\n\n        keep_cols = [\"location_id\", \"location_name\", \"most_detailed\", \"parent_id\"]\n        keep_mask = (\n            (pop.year_id == pop.year_id.max())  # Year doesn't matter\n            &amp; (pop.most_detailed == 1)\n        )\n        out = gdf.merge(pop.loc[keep_mask, keep_cols], on=\"location_id\", how=\"left\")\n    elif full_aggregation_hierarchy in [\"lsae_1209\", \"lsae_1285\"]:\n        # This is only a2 geoms, so already most detailed\n        shape_path = (\n            root\n            / \"gbd-inputs\"\n            / f\"shapes_{full_aggregation_hierarchy}_a2.parquet\"\n        )\n        out = gpd.read_parquet(shape_path, bbox=bounds)\n    else:\n        msg = f\"Unknown pixel hierarchy: {full_aggregation_hierarchy}\"\n        raise ValueError(msg)\n    return out\n</code></pre>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster","title":"<code>to_raster(ds: xr.DataArray, no_data_value: float | int, lat_col: str = 'lat', lon_col: str = 'lon', crs: str = 'EPSG:4326') -&gt; rt.RasterArray</code>","text":"<p>Convert an xarray DataArray to a RasterArray.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--parameters","title":"Parameters","text":"<p>ds     The xarray DataArray to convert. no_data_value     The value to use for missing data. This should be consistent with the dtype of the data. lat_col     The name of the latitude coordinate in the dataset. lon_col     The name of the longitude coordinate in the dataset. crs     The coordinate reference system of the data.</p>"},{"location":"reference/idd_forecast_mbp/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--returns","title":"Returns","text":"<p>rt.RasterArray     The RasterArray representation of the input data.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def to_raster(\n    ds: xr.DataArray,\n    no_data_value: float | int,\n    lat_col: str = \"lat\",\n    lon_col: str = \"lon\",\n    crs: str = \"EPSG:4326\",\n) -&gt; rt.RasterArray:\n    \"\"\"Convert an xarray DataArray to a RasterArray.\n\n    Parameters\n    ----------\n    ds\n        The xarray DataArray to convert.\n    no_data_value\n        The value to use for missing data. This should be consistent with the dtype of the data.\n    lat_col\n        The name of the latitude coordinate in the dataset.\n    lon_col\n        The name of the longitude coordinate in the dataset.\n    crs\n        The coordinate reference system of the data.\n\n    Returns\n    -------\n    rt.RasterArray\n        The RasterArray representation of the input data.\n    \"\"\"\n    lat, lon = ds[lat_col].data, ds[lon_col].data\n\n    dlat = (lat[1:] - lat[:-1]).mean()\n    dlon = (lon[1:] - lon[:-1]).mean()\n\n    transform = Affine(\n        a=dlon,\n        b=0.0,\n        c=lon[0],\n        d=0.0,\n        e=-dlat,\n        f=lat[-1],\n    )\n    return rt.RasterArray(\n        data=ds.data[::-1],\n        transform=transform,\n        crs=crs,\n        no_data_value=no_data_value,\n    )\n</code></pre>"},{"location":"reference/idd_forecast_mbp/cli/","title":"cli","text":""},{"location":"reference/idd_forecast_mbp/constants/","title":"constants","text":""},{"location":"reference/idd_forecast_mbp/data/","title":"data","text":""},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData","title":"<code>FloodingData</code>","text":"<p>Class to handle flooding data.</p> Source code in <code>src/idd_forecast_mbp/data.py</code> <pre><code>class FloodingData:\n    \"\"\"\n    Class to handle flooding data.\n    \"\"\"\n\n    def __init__(self, root: str | Path = rfc.MODEL_ROOT):\n        self._root = Path(root)\n\n    @property\n    def root(self) -&gt; Path:\n        \"\"\"\n        Returns the root path of the flooding data.\n        \"\"\"\n        return self._root\n\n    @property\n    def logs(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the logs directory.\n        \"\"\"\n        return self.root / \"logs\"\n\n    def log_dir(self, step_name: str) -&gt; Path:\n        return self.logs / step_name\n\n    @property\n    def raw_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"01-raw_data\"\n\n    @property\n    def processed_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"02-processed_data\"\n\n    @property\n    def modeling_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"03-modeling_data\"\n\n    @property\n    def forecasting_root(self) -&gt; Path:\n        \"\"\"\n        Returns the path to the CaMa-Flood root directory.\n        \"\"\"\n        return self.root / \"04-forecasting_data\"\n</code></pre>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.forecasting_root","title":"<code>forecasting_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.logs","title":"<code>logs: Path</code>  <code>property</code>","text":"<p>Returns the path to the logs directory.</p>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.modeling_root","title":"<code>modeling_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.processed_root","title":"<code>processed_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.raw_root","title":"<code>raw_root: Path</code>  <code>property</code>","text":"<p>Returns the path to the CaMa-Flood root directory.</p>"},{"location":"reference/idd_forecast_mbp/data/#idd_forecast_mbp.data.FloodingData.root","title":"<code>root: Path</code>  <code>property</code>","text":"<p>Returns the root path of the flooding data.</p>"},{"location":"reference/idd_forecast_mbp/helper_functions/","title":"helper_functions","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/","title":"map_to_admin_2","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps","title":"<code>01_prep_maps</code>","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates","title":"<code>batch_process_all_covariates(output_dir=None, skip_existing=True)</code>","text":"<p>Process all covariates in the COVARIATE_DICT and convert them to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--parameters","title":"Parameters:","text":"<p>output_dir : str or Path, optional     Directory to save output files. If None, uses default directory. skip_existing : bool, default=True     If True, skip processing covariates that already have output files.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--returns","title":"Returns:","text":"<p>dict: Dictionary with covariate names as keys and output paths as values</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def batch_process_all_covariates(output_dir=None, skip_existing=True):\n    \"\"\"\n    Process all covariates in the COVARIATE_DICT and convert them to netCDF.\n\n    Parameters:\n    -----------\n    output_dir : str or Path, optional\n        Directory to save output files. If None, uses default directory.\n    skip_existing : bool, default=True\n        If True, skip processing covariates that already have output files.\n\n    Returns:\n    --------\n    dict: Dictionary with covariate names as keys and output paths as values\n    \"\"\"\n    if output_dir is None:\n        output_dir = OUTPUT_PATH\n    else:\n        output_dir = Path(output_dir)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    results = {}\n\n    for covariate_key in COVARIATE_DICT.keys():\n        print(f\"\\nProcessing {covariate_key}...\")\n        covariate_dict = parse_yaml_dictionary(covariate_key)\n        covariate_name = covariate_dict['covariate_name']\n\n        output_path = output_dir / f\"{covariate_name}.nc\"\n\n        # Skip if output file already exists and skip_existing is True\n        if skip_existing and os.path.exists(output_path):\n            print(f\"Output file {output_path} already exists. Skipping.\")\n            results[covariate_name] = str(output_path)\n            continue\n\n        try:\n            synoptic = covariate_dict['synoptic']\n            years = covariate_dict['years']\n\n            if synoptic or len(years) == 1:\n                # Process as synoptic variable\n                results[covariate_name] = process_synoptic_variable(covariate_key, output_dir)\n            else:\n                # Process as multi-year variable\n                results[covariate_name] = process_multiyear_variable(covariate_key, output_dir)\n        except Exception as e:\n            print(f\"Error processing {covariate_name}: {str(e)}\")\n            results[covariate_name] = None\n\n    # Display summary\n    print(\"\\n==== Processing Summary ====\")\n    success_count = sum(1 for path in results.values() if path is not None)\n    print(f\"Successfully processed {success_count} out of {len(results)} covariates\")\n\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf","title":"<code>create_multiyear_netcdf(input_path_template, years, output_path, variable_name)</code>","text":"<p>Create a multi-year netCDF file from individual GeoTIFF files.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--parameters","title":"Parameters:","text":"<p>input_path_template : str     Template path with {year} to be replaced for each year years : list of int     List of years to process output_path : str     Path to save the combined netCDF file variable_name : str     Name of the variable in the output file</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset with time dimension</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def create_multiyear_netcdf(input_path_template, years, output_path, variable_name):\n    \"\"\"\n    Create a multi-year netCDF file from individual GeoTIFF files.\n\n    Parameters:\n    -----------\n    input_path_template : str\n        Template path with {year} to be replaced for each year\n    years : list of int\n        List of years to process\n    output_path : str\n        Path to save the combined netCDF file\n    variable_name : str\n        Name of the variable in the output file\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset with time dimension\n    \"\"\"\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = input_path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(input_path, temp_path, variable_name, year=year)\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    # Sort by time\n    if combined_ds is not None:\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Update time attributes to ensure it's treated as simple year integers\n        if combined_ds is not None:\n            combined_ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\", \n                \"units\": \"year\"\n            }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        return combined_ds\n    else:\n        print(\"No valid data files found.\")\n        return None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf","title":"<code>geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None, global_extent=True, fill_value=0.0)</code>","text":"<p>Convert a GeoTIFF file to netCDF format.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--parameters","title":"Parameters:","text":"<p>input_path : str     Path to the input GeoTIFF file output_path : str     Path to save the output netCDF file variable_name : str     Name of the variable to use in the netCDF file resolution : float, optional     Resolution of the raster in degrees. If None, will be calculated from the transform. year : int, optional     Year for the data (for time dimension) encoding : dict, optional     Encoding options for xarray.to_netcdf() global_extent : bool, default=False     If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90) fill_value : float, default=0.0     Value to fill extended areas with</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None,\n                      global_extent=True, fill_value=0.0):\n    \"\"\"\n    Convert a GeoTIFF file to netCDF format.\n\n    Parameters:\n    -----------\n    input_path : str\n        Path to the input GeoTIFF file\n    output_path : str\n        Path to save the output netCDF file\n    variable_name : str\n        Name of the variable to use in the netCDF file\n    resolution : float, optional\n        Resolution of the raster in degrees. If None, will be calculated from the transform.\n    year : int, optional\n        Year for the data (for time dimension)\n    encoding : dict, optional\n        Encoding options for xarray.to_netcdf()\n    global_extent : bool, default=False\n        If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90)\n    fill_value : float, default=0.0\n        Value to fill extended areas with\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Open the GeoTIFF file\n    with rio.open(input_path) as src:\n        # Read the data and transform information\n        data = src.read(1)  # Read first band\n        height, width = data.shape\n\n        # Handle no-data values\n        if src.nodata is not None:\n            data = np.where(data == src.nodata, np.nan, data)\n\n        # Get geospatial information\n        transform = src.transform\n\n        # Use provided resolution or calculate it from transform\n        if resolution is None:\n            x_res = transform[0]  # Width of a pixel\n            y_res = abs(transform[4])  # Height of a pixel\n        else:\n            x_res = y_res = resolution  # Use the square pixels resolution from YAML\n\n        # Calculate bounds correctly with full pixel coverage\n        xmin = transform[2]  # Left edge\n        ymax = transform[5]  # Top edge\n\n        # Use resolution to calculate exact right and bottom edges\n        xmax = xmin + (width * x_res)  # Right edge (including full pixel width)\n        ymin = ymax - (height * y_res)  # Bottom edge (including full pixel height)\n\n        if global_extent:\n            # Define global extents\n            global_xmin, global_xmax = -180.0, 180.0\n            global_ymin, global_ymax = -90.0, 90.0\n\n            # Calculate number of pixels needed for global coverage\n            global_width = int(np.ceil((global_xmax - global_xmin) / x_res))\n            global_height = int(np.ceil((global_ymax - global_ymin) / y_res))\n\n            # Create empty global array filled with fill_value\n            global_data = np.full((global_height, global_width), fill_value, dtype=data.dtype)\n\n            # Calculate indices to place original data\n            x_start = int(round((xmin - global_xmin) / x_res))\n            y_start = int(round((global_ymax - ymax) / y_res))\n\n            # Ensure indices are within bounds\n            x_start = max(0, x_start)\n            y_start = max(0, y_start)\n\n            # Place the original data into the global grid\n            x_end = min(x_start + width, global_width)\n            y_end = min(y_start + height, global_height)\n\n            # Calculate how much of the original data we can fit\n            orig_x_slice = slice(0, x_end - x_start)\n            orig_y_slice = slice(0, y_end - y_start)\n\n            # Place the data\n            global_data[y_start:y_end, x_start:x_end] = data[orig_y_slice, orig_x_slice]\n\n            # Use this data for the rest of the function\n            data = global_data\n            xmin, xmax = global_xmin, global_xmax\n            ymin, ymax = global_ymin, global_ymax\n            width, height = global_width, global_height\n\n        # Create coordinate arrays for pixel centers (not edges)\n        lons = np.linspace(xmin + x_res/2, xmax - x_res/2, width)\n        lats = np.linspace(ymax - y_res/2, ymin + y_res/2, height)\n\n        # Create xarray dataset\n        if year is not None:\n            # Create a dataset with time dimension using integer year\n            time_val = np.array([int(year)])  # Ensure it's an integer in an array\n            ds = xr.Dataset(\n                {\"value\": ([\"time\", \"lat\", \"lon\"], data[np.newaxis, :, :])},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"time\": time_val\n                }\n            )\n\n            # Add time coordinate attributes indicating it's just a year, not a timestamp\n            ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\",\n                \"units\": \"year\"\n            }\n        else:\n            # Create a dataset without time dimension (synoptic)\n            ds = xr.Dataset(\n                {\"value\": ([\"lat\", \"lon\"], data)},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats\n                }\n            )\n\n        # Add variable attributes\n        ds[\"value\"].attrs = {\n            \"long_name\": variable_name.replace(\"_\", \" \"),\n            \"units\": src.meta.get(\"units\", \"unknown\"),\n            \"resolution\": x_res  # Add resolution to metadata\n        }\n\n        # Add coordinate attributes\n        ds.lon.attrs = {\n            \"long_name\": \"longitude\",\n            \"standard_name\": \"longitude\",\n            \"units\": \"degrees_east\",\n            \"axis\": \"X\",\n            \"resolution\": x_res\n        }\n\n        ds.lat.attrs = {\n            \"long_name\": \"latitude\",\n            \"standard_name\": \"latitude\",\n            \"units\": \"degrees_north\",\n            \"axis\": \"Y\",\n            \"resolution\": y_res\n        }\n\n        # Add global attributes\n        ds.attrs = {\n            \"title\": f\"{variable_name} data\",\n            \"source\": f\"Converted from GeoTIFF: {os.path.basename(input_path)}\",\n            \"created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"Conventions\": \"CF-1.8\",\n            \"history\": f\"Created from {input_path}\",\n            \"pixel_size_degrees\": x_res,\n            \"pixel_registration\": \"center\"\n        }\n\n        # Use default encoding if none provided\n        if encoding is None:\n            encoding = {\n                \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n                \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n                \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5}\n            }\n            if year is not None:\n                # Ensure time is stored as a simple integer with no date units\n                encoding[\"time\"] = {\"dtype\": \"int32\"}\n\n        # Save the dataset to netCDF\n        ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n        return ds\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.main","title":"<code>main()</code>","text":"<p>Main function to run when script is executed directly.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def main():\n    \"\"\"Main function to run when script is executed directly.\"\"\"\n    results = batch_process_all_covariates(skip_existing=False)\n\n    # Print successful conversions\n    print(\"\\nSuccessfully converted covariates:\")\n    for covariate, path in results.items():\n        if path:\n            print(f\"- {covariate}: {path}\")\n\n    # Print failed conversions\n    print(\"\\nFailed conversions:\")\n    for covariate, path in results.items():\n        if not path:\n            print(f\"- {covariate}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable","title":"<code>process_multiyear_variable(covariate_key, output_dir)</code>","text":"<p>Process a multi-year variable and convert to a single netCDF with time dimension.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_multiyear_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a multi-year variable and convert to a single netCDF with time dimension.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path_template = covariate_dict['path']\n    years = covariate_dict['years']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    if not years:\n        print(f\"No years specified for {covariate_name}. Cannot process as multi-year variable.\")\n        return None\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Update create_multiyear_netcdf to pass resolution to geotiff_to_netcdf\n    # This requires modifying the create_multiyear_netcdf function\n\n    # For now, pass resolution directly to each geotiff_to_netcdf call\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(\n            input_path=input_path, \n            output_path=temp_path, \n            variable_name=covariate_name, \n            resolution=resolution,\n            year=year\n        )\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    if combined_ds is not None:\n        # Sort by time\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n    return str(output_path) if combined_ds is not None else None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable","title":"<code>process_synoptic_variable(covariate_key, output_dir)</code>","text":"<p>Process a synoptic (time-invariant) variable and convert to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_synoptic_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a synoptic (time-invariant) variable and convert to netCDF.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path = covariate_dict['path']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Convert to netCDF with resolution from YAML\n    geotiff_to_netcdf(\n        input_path=path,\n        output_path=str(output_path),\n        variable_name=covariate_name,\n        resolution=resolution\n    )\n\n    return str(output_path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.set_file_permissions","title":"<code>set_file_permissions(file_path)</code>","text":"<p>Set 775 permissions (rwxrwxr-x) on the specified file.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def set_file_permissions(file_path):\n    \"\"\"Set 775 permissions (rwxrwxr-x) on the specified file.\"\"\"\n    try:\n        os.chmod(file_path, 0o775)  # 0o775 is octal for 775 permissions\n    except Exception as e:\n        print(f\"Warning: Could not set permissions on {file_path}: {str(e)}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy","title":"<code>pixel_hierarchy</code>","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy","title":"<code>aggregate_climate_to_hierarchy(data: pd.DataFrame, hierarchy: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Create all aggregate climate values for a given hierarchy from most-detailed data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--parameters","title":"Parameters","text":"<p>data     The most-detailed climate data to aggregate. hierarchy     The hierarchy to aggregate the data to.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The climate data with values for all levels of the hierarchy.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def aggregate_climate_to_hierarchy(\n    data: pd.DataFrame, hierarchy: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Create all aggregate climate values for a given hierarchy from most-detailed data.\n\n    Parameters\n    ----------\n    data\n        The most-detailed climate data to aggregate.\n    hierarchy\n        The hierarchy to aggregate the data to.\n\n    Returns\n    -------\n    pd.DataFrame\n        The climate data with values for all levels of the hierarchy.\n    \"\"\"\n    results = data.set_index(\"location_id\").copy()\n\n    # Most detailed locations can be at multiple levels of the hierarchy,\n    # so we loop over all levels from most detailed to global, aggregating\n    # level by level and appending the results to the data.\n    for level in reversed(list(range(1, hierarchy.level.max() + 1))):\n        level_mask = hierarchy.level == level\n        parent_map = hierarchy.loc[level_mask].set_index(\"location_id\").parent_id\n\n        # For every location in the parent map, we need to check if it is the results\n        # For those that are, proceed to aggregate\n        # For those that aren't, check to make sure their parent is in the results. If not, exit with an error\n        absent_parent_map = parent_map.index.difference(results.index)\n        if len(absent_parent_map) &gt; 0:\n            msg = f\"Some parent locations are not in the results: {absent_parent_map}\"\n            # Check to see if the parent of each location id that is missing is in the results\n            parent_of_absent = parent_map.loc[absent_parent_map]\n            unique_parent_ids = parent_of_absent.unique()\n            # Check to see if the unique_parent_ids are in the results\n            missing_parents = unique_parent_ids[~np.isin(unique_parent_ids, results.index)]\n            if len(missing_parents) &gt; 0:\n                msg = f\"Some parent locations are not in the results: {missing_parents}\"\n                raise ValueError(msg)\n\n        present_parent_map = parent_map.loc[parent_map.index.isin(results.index)]\n        # Continue aggregation only on the present locations\n        subset = results.loc[present_parent_map.index]\n        subset[\"parent_id\"] = present_parent_map\n\n        parent_values = (\n            subset.groupby([\"year_id\", \"parent_id\"])[[\"weighted_climate\", \"population\"]]\n            .sum()\n            .reset_index()\n            .rename(columns={\"parent_id\": \"location_id\"})\n            .set_index(\"location_id\")\n        )\n        results = pd.concat([results, parent_values])\n    results = (\n        results.reset_index()\n        .sort_values([\"location_id\", \"year_id\"])\n    )\n    parent_values[\"value\"] = parent_values.weighted_climate / parent_values.population\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy","title":"<code>load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame</code>","text":"<p>Load a subset location hierarchy.</p> <p>The subset hierarchy might be equal to the full aggregation hierarchy, but it might also be a subset of the full aggregation hierarchy. These hierarchies are used to provide different views of aggregated climate data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--parameters","title":"Parameters","text":"<p>subset_hierarchy     The administrative hierarchy to load (e.g. \"gbd_2021\")</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The hierarchy data with parent-child relationships</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame:\n    \"\"\"Load a subset location hierarchy.\n\n    The subset hierarchy might be equal to the full aggregation hierarchy,\n    but it might also be a subset of the full aggregation hierarchy.\n    These hierarchies are used to provide different views of aggregated\n    climate data.\n\n    Parameters\n    ----------\n    subset_hierarchy\n        The administrative hierarchy to load (e.g. \"gbd_2021\")\n\n    Returns\n    -------\n    pd.DataFrame\n        The hierarchy data with parent-child relationships\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    allowed_hierarchies = [\"gbd_2021\", \"fhs_2021\", \"lsae_1209\", \"lsae_1285\"]\n    if subset_hierarchy not in allowed_hierarchies:\n        msg = f\"Unknown admin hierarchy: {subset_hierarchy}\"\n        raise ValueError(msg)\n    path = root / \"gbd-inputs\" / f\"hierarchy_{subset_hierarchy}.parquet\"\n    return pd.read_parquet(path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.post_process","title":"<code>post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Rename 000 to {summary_covariate}_per_capita Merge in population Create {summary_covariate}_capita*population -&gt; {summary_covariate}</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame: # Fix this for other summary_variable/variable/etc\n    \"\"\"\n    Rename 000 to {summary_covariate}_per_capita\n    Merge in population\n    Create {summary_covariate}_capita*population -&gt; {summary_covariate}\n    \"\"\"\n\n    # Rename 000 to people_flood_days_per_capita\n    df = df.rename(columns={\"000\": f\"{summary_covariate}_per_capita\"})\n\n    # Merge in population\n    full_df = df.merge(\n        pop_df,\n        on=[\"location_id\", \"year_id\"],\n        how=\"left\",\n    )\n    # assert all location_ids and years combinations are present\n    assert df.shape[0] == full_df.shape[0]\n    assert df.location_id.nunique() == full_df.location_id.nunique()\n    assert df.year_id.nunique() == full_df.year_id.nunique()\n\n    # Create {summary_covariate}\n    full_df[summary_covariate] = (\n        full_df[f\"{summary_covariate}_per_capita\"] * full_df[\"population\"]\n    ).astype(np.float32)\n\n    return full_df\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main","title":"<code>pixel_main</code>","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map","title":"<code>build_bounds_map(raster_template: rt.RasterArray, shape_values: list[tuple[Polygon | MultiPolygon, int]]) -&gt; dict[int, tuple[slice, slice]]</code>","text":"<p>Build a map of location IDs to buffered slices of the raster template.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--parameters","title":"Parameters","text":"<p>raster_template     The raster template to build the bounds map for. shape_values     A list of tuples where the first element is a shapely Polygon or MultiPolygon     in the CRS of the raster template and the second element is the location ID     of the shape.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--returns","title":"Returns","text":"<p>dict[int, tuple[slice, slice]]     A dictionary mapping location IDs to a tuple of slices representing the bounds     of the location in the raster template. The slices are buffered by 10 pixels     to ensure that the entire shape is included in the mask.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def build_bounds_map(\n    raster_template: rt.RasterArray,\n    shape_values: list[tuple[Polygon | MultiPolygon, int]],\n) -&gt; dict[int, tuple[slice, slice]]:\n    \"\"\"Build a map of location IDs to buffered slices of the raster template.\n\n    Parameters\n    ----------\n    raster_template\n        The raster template to build the bounds map for.\n    shape_values\n        A list of tuples where the first element is a shapely Polygon or MultiPolygon\n        in the CRS of the raster template and the second element is the location ID\n        of the shape.\n\n    Returns\n    -------\n    dict[int, tuple[slice, slice]]\n        A dictionary mapping location IDs to a tuple of slices representing the bounds\n        of the location in the raster template. The slices are buffered by 10 pixels\n        to ensure that the entire shape is included in the mask.\n    \"\"\"\n    # The tranform maps pixel coordinates to the CRS coordinates.\n    # This mask is the inverse of that transform.\n    to_pixel = ~raster_template.transform\n\n    bounds_map = {}\n    for shp, loc_id in shape_values:\n        xmin, ymin, xmax, ymax = shp.bounds\n        pxmin, pymin = to_pixel * (xmin, ymax)\n        pixel_buffer = 10\n        pxmin = max(0, int(pxmin) - pixel_buffer)\n        pymin = max(0, int(pymin) - pixel_buffer)\n        pxmax, pymax = to_pixel * (xmax, ymin)\n        pxmax = min(raster_template.width, int(pxmax) + pixel_buffer)\n        pymax = min(raster_template.height, int(pymax) + pixel_buffer)\n        bounds_map[loc_id] = (slice(pymin, pymax), slice(pxmin, pxmax))\n\n    return bounds_map\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox","title":"<code>get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon</code>","text":"<p>Get the bounding box of a raster array.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--parameters","title":"Parameters","text":"<p>raster     The raster array to get the bounding box of. crs     The CRS to return the bounding box in. If None, the bounding box     is returned in the CRS of the raster.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--returns","title":"Returns","text":"<p>shapely.Polybon     The bounding box of the raster in the CRS specified by the crs parameter.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon:\n    \"\"\"Get the bounding box of a raster array.\n\n    Parameters\n    ----------\n    raster\n        The raster array to get the bounding box of.\n    crs\n        The CRS to return the bounding box in. If None, the bounding box\n        is returned in the CRS of the raster.\n\n    Returns\n    -------\n    shapely.Polybon\n        The bounding box of the raster in the CRS specified by the crs parameter.\n    \"\"\"\n    if raster.crs not in MAX_BOUNDS:\n        msg = f\"Unsupported CRS: {raster.crs}\"\n        raise ValueError(msg)\n\n    xmin_clip, xmax_clip, ymin_clip, ymax_clip = MAX_BOUNDS[raster.crs]\n    xmin, xmax, ymin, ymax = raster.bounds\n\n    xmin = np.clip(xmin, xmin_clip, xmax_clip)\n    xmax = np.clip(xmax, xmin_clip, xmax_clip)\n    ymin = np.clip(ymin, ymin_clip, ymax_clip)\n    ymax = np.clip(ymax, ymin_clip, ymax_clip)\n\n    bbox = gpd.GeoSeries([shapely.box(xmin, ymin, xmax, ymax)], crs=raster.crs)\n    out_bbox = bbox.to_crs(crs) if crs is not None else bbox.copy()\n\n    # Check that our transformation didn't do something weird\n    # (e.g. artificially clip the bounds or have the bounds extend over the\n    # antimeridian)\n    check_bbox = out_bbox.to_crs(raster.crs)\n    area_change = (np.abs(bbox.area - check_bbox.area) / bbox.area).iloc[0]\n    tolerance = 1e-6\n    if area_change &gt; tolerance:\n        msg = f\"Area change: {area_change}\"\n        raise ValueError(msg)\n\n    return cast(shapely.Polygon, out_bbox.iloc[0])\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes","title":"<code>load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]) -&gt; gpd.GeoDataFrame</code>","text":"<p>Load shapes for a full aggregation hierarchy within given bounds.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--parameters","title":"Parameters","text":"<p>full_aggregation_hierarchy     The full aggregation hierarchy to load (e.g. \"gbd_2021\") bounds     The bounds to load (xmin, ymin, xmax, ymax)</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--returns","title":"Returns","text":"<p>gpd.GeoDataFrame     The shapes for the given hierarchy and bounds</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load shapes for a full aggregation hierarchy within given bounds.\n\n    Parameters\n    ----------\n    full_aggregation_hierarchy\n        The full aggregation hierarchy to load (e.g. \"gbd_2021\")\n    bounds\n        The bounds to load (xmin, ymin, xmax, ymax)\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        The shapes for the given hierarchy and bounds\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    if full_aggregation_hierarchy in [\"gbd_2021\", \"gbd_2023\"]:\n        shape_path = (\n            root/ f\"shapes_{full_aggregation_hierarchy}.parquet\"\n        )\n        gdf = gpd.read_parquet(shape_path, bbox=bounds)\n\n        # We're using population data here instead of a hierarchy because\n        # The populations include extra locations we've supplemented that aren't\n        # modeled in GBD (e.g. locations with zero population or places that\n        # GBD uses population scalars from WPP to model)\n        pop_path = (\n            root / f\"population_{full_aggregation_hierarchy}.parquet\"\n        )\n        pop = pd.read_parquet(pop_path)\n\n        keep_cols = [\"location_id\", \"location_name\", \"most_detailed\", \"parent_id\"]\n        keep_mask = (\n            (pop.year_id == pop.year_id.max())  # Year doesn't matter\n            &amp; (pop.most_detailed == 1)\n        )\n        out = gdf.merge(pop.loc[keep_mask, keep_cols], on=\"location_id\", how=\"left\")\n    elif full_aggregation_hierarchy in [\"lsae_1209\", \"lsae_1285\"]:\n        # This is only a2 geoms, so already most detailed\n        shape_path = (\n            root\n            / \"gbd-inputs\"\n            / f\"shapes_{full_aggregation_hierarchy}_a2.parquet\"\n        )\n        out = gpd.read_parquet(shape_path, bbox=bounds)\n    else:\n        msg = f\"Unknown pixel hierarchy: {full_aggregation_hierarchy}\"\n        raise ValueError(msg)\n    return out\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster","title":"<code>to_raster(ds: xr.DataArray, no_data_value: float | int, lat_col: str = 'lat', lon_col: str = 'lon', crs: str = 'EPSG:4326') -&gt; rt.RasterArray</code>","text":"<p>Convert an xarray DataArray to a RasterArray.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--parameters","title":"Parameters","text":"<p>ds     The xarray DataArray to convert. no_data_value     The value to use for missing data. This should be consistent with the dtype of the data. lat_col     The name of the latitude coordinate in the dataset. lon_col     The name of the longitude coordinate in the dataset. crs     The coordinate reference system of the data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--returns","title":"Returns","text":"<p>rt.RasterArray     The RasterArray representation of the input data.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def to_raster(\n    ds: xr.DataArray,\n    no_data_value: float | int,\n    lat_col: str = \"lat\",\n    lon_col: str = \"lon\",\n    crs: str = \"EPSG:4326\",\n) -&gt; rt.RasterArray:\n    \"\"\"Convert an xarray DataArray to a RasterArray.\n\n    Parameters\n    ----------\n    ds\n        The xarray DataArray to convert.\n    no_data_value\n        The value to use for missing data. This should be consistent with the dtype of the data.\n    lat_col\n        The name of the latitude coordinate in the dataset.\n    lon_col\n        The name of the longitude coordinate in the dataset.\n    crs\n        The coordinate reference system of the data.\n\n    Returns\n    -------\n    rt.RasterArray\n        The RasterArray representation of the input data.\n    \"\"\"\n    lat, lon = ds[lat_col].data, ds[lon_col].data\n\n    dlat = (lat[1:] - lat[:-1]).mean()\n    dlon = (lon[1:] - lon[:-1]).mean()\n\n    transform = Affine(\n        a=dlon,\n        b=0.0,\n        c=lon[0],\n        d=0.0,\n        e=-dlat,\n        f=lat[-1],\n    )\n    return rt.RasterArray(\n        data=ds.data[::-1],\n        transform=transform,\n        crs=crs,\n        no_data_value=no_data_value,\n    )\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/","title":"01_prep_maps","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates","title":"<code>batch_process_all_covariates(output_dir=None, skip_existing=True)</code>","text":"<p>Process all covariates in the COVARIATE_DICT and convert them to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--parameters","title":"Parameters:","text":"<p>output_dir : str or Path, optional     Directory to save output files. If None, uses default directory. skip_existing : bool, default=True     If True, skip processing covariates that already have output files.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.batch_process_all_covariates--returns","title":"Returns:","text":"<p>dict: Dictionary with covariate names as keys and output paths as values</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def batch_process_all_covariates(output_dir=None, skip_existing=True):\n    \"\"\"\n    Process all covariates in the COVARIATE_DICT and convert them to netCDF.\n\n    Parameters:\n    -----------\n    output_dir : str or Path, optional\n        Directory to save output files. If None, uses default directory.\n    skip_existing : bool, default=True\n        If True, skip processing covariates that already have output files.\n\n    Returns:\n    --------\n    dict: Dictionary with covariate names as keys and output paths as values\n    \"\"\"\n    if output_dir is None:\n        output_dir = OUTPUT_PATH\n    else:\n        output_dir = Path(output_dir)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    results = {}\n\n    for covariate_key in COVARIATE_DICT.keys():\n        print(f\"\\nProcessing {covariate_key}...\")\n        covariate_dict = parse_yaml_dictionary(covariate_key)\n        covariate_name = covariate_dict['covariate_name']\n\n        output_path = output_dir / f\"{covariate_name}.nc\"\n\n        # Skip if output file already exists and skip_existing is True\n        if skip_existing and os.path.exists(output_path):\n            print(f\"Output file {output_path} already exists. Skipping.\")\n            results[covariate_name] = str(output_path)\n            continue\n\n        try:\n            synoptic = covariate_dict['synoptic']\n            years = covariate_dict['years']\n\n            if synoptic or len(years) == 1:\n                # Process as synoptic variable\n                results[covariate_name] = process_synoptic_variable(covariate_key, output_dir)\n            else:\n                # Process as multi-year variable\n                results[covariate_name] = process_multiyear_variable(covariate_key, output_dir)\n        except Exception as e:\n            print(f\"Error processing {covariate_name}: {str(e)}\")\n            results[covariate_name] = None\n\n    # Display summary\n    print(\"\\n==== Processing Summary ====\")\n    success_count = sum(1 for path in results.values() if path is not None)\n    print(f\"Successfully processed {success_count} out of {len(results)} covariates\")\n\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf","title":"<code>create_multiyear_netcdf(input_path_template, years, output_path, variable_name)</code>","text":"<p>Create a multi-year netCDF file from individual GeoTIFF files.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--parameters","title":"Parameters:","text":"<p>input_path_template : str     Template path with {year} to be replaced for each year years : list of int     List of years to process output_path : str     Path to save the combined netCDF file variable_name : str     Name of the variable in the output file</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.create_multiyear_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset with time dimension</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def create_multiyear_netcdf(input_path_template, years, output_path, variable_name):\n    \"\"\"\n    Create a multi-year netCDF file from individual GeoTIFF files.\n\n    Parameters:\n    -----------\n    input_path_template : str\n        Template path with {year} to be replaced for each year\n    years : list of int\n        List of years to process\n    output_path : str\n        Path to save the combined netCDF file\n    variable_name : str\n        Name of the variable in the output file\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset with time dimension\n    \"\"\"\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = input_path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(input_path, temp_path, variable_name, year=year)\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    # Sort by time\n    if combined_ds is not None:\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Update time attributes to ensure it's treated as simple year integers\n        if combined_ds is not None:\n            combined_ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\", \n                \"units\": \"year\"\n            }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        return combined_ds\n    else:\n        print(\"No valid data files found.\")\n        return None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf","title":"<code>geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None, global_extent=True, fill_value=0.0)</code>","text":"<p>Convert a GeoTIFF file to netCDF format.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--parameters","title":"Parameters:","text":"<p>input_path : str     Path to the input GeoTIFF file output_path : str     Path to save the output netCDF file variable_name : str     Name of the variable to use in the netCDF file resolution : float, optional     Resolution of the raster in degrees. If None, will be calculated from the transform. year : int, optional     Year for the data (for time dimension) encoding : dict, optional     Encoding options for xarray.to_netcdf() global_extent : bool, default=False     If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90) fill_value : float, default=0.0     Value to fill extended areas with</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.geotiff_to_netcdf--returns","title":"Returns:","text":"<p>xr.Dataset: The created dataset</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def geotiff_to_netcdf(input_path, output_path, variable_name, resolution=None, year=None, encoding=None,\n                      global_extent=True, fill_value=0.0):\n    \"\"\"\n    Convert a GeoTIFF file to netCDF format.\n\n    Parameters:\n    -----------\n    input_path : str\n        Path to the input GeoTIFF file\n    output_path : str\n        Path to save the output netCDF file\n    variable_name : str\n        Name of the variable to use in the netCDF file\n    resolution : float, optional\n        Resolution of the raster in degrees. If None, will be calculated from the transform.\n    year : int, optional\n        Year for the data (for time dimension)\n    encoding : dict, optional\n        Encoding options for xarray.to_netcdf()\n    global_extent : bool, default=False\n        If True, extend the raster to global lat/lon coverage (-180 to 180, -90 to 90)\n    fill_value : float, default=0.0\n        Value to fill extended areas with\n\n    Returns:\n    --------\n    xr.Dataset: The created dataset\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Open the GeoTIFF file\n    with rio.open(input_path) as src:\n        # Read the data and transform information\n        data = src.read(1)  # Read first band\n        height, width = data.shape\n\n        # Handle no-data values\n        if src.nodata is not None:\n            data = np.where(data == src.nodata, np.nan, data)\n\n        # Get geospatial information\n        transform = src.transform\n\n        # Use provided resolution or calculate it from transform\n        if resolution is None:\n            x_res = transform[0]  # Width of a pixel\n            y_res = abs(transform[4])  # Height of a pixel\n        else:\n            x_res = y_res = resolution  # Use the square pixels resolution from YAML\n\n        # Calculate bounds correctly with full pixel coverage\n        xmin = transform[2]  # Left edge\n        ymax = transform[5]  # Top edge\n\n        # Use resolution to calculate exact right and bottom edges\n        xmax = xmin + (width * x_res)  # Right edge (including full pixel width)\n        ymin = ymax - (height * y_res)  # Bottom edge (including full pixel height)\n\n        if global_extent:\n            # Define global extents\n            global_xmin, global_xmax = -180.0, 180.0\n            global_ymin, global_ymax = -90.0, 90.0\n\n            # Calculate number of pixels needed for global coverage\n            global_width = int(np.ceil((global_xmax - global_xmin) / x_res))\n            global_height = int(np.ceil((global_ymax - global_ymin) / y_res))\n\n            # Create empty global array filled with fill_value\n            global_data = np.full((global_height, global_width), fill_value, dtype=data.dtype)\n\n            # Calculate indices to place original data\n            x_start = int(round((xmin - global_xmin) / x_res))\n            y_start = int(round((global_ymax - ymax) / y_res))\n\n            # Ensure indices are within bounds\n            x_start = max(0, x_start)\n            y_start = max(0, y_start)\n\n            # Place the original data into the global grid\n            x_end = min(x_start + width, global_width)\n            y_end = min(y_start + height, global_height)\n\n            # Calculate how much of the original data we can fit\n            orig_x_slice = slice(0, x_end - x_start)\n            orig_y_slice = slice(0, y_end - y_start)\n\n            # Place the data\n            global_data[y_start:y_end, x_start:x_end] = data[orig_y_slice, orig_x_slice]\n\n            # Use this data for the rest of the function\n            data = global_data\n            xmin, xmax = global_xmin, global_xmax\n            ymin, ymax = global_ymin, global_ymax\n            width, height = global_width, global_height\n\n        # Create coordinate arrays for pixel centers (not edges)\n        lons = np.linspace(xmin + x_res/2, xmax - x_res/2, width)\n        lats = np.linspace(ymax - y_res/2, ymin + y_res/2, height)\n\n        # Create xarray dataset\n        if year is not None:\n            # Create a dataset with time dimension using integer year\n            time_val = np.array([int(year)])  # Ensure it's an integer in an array\n            ds = xr.Dataset(\n                {\"value\": ([\"time\", \"lat\", \"lon\"], data[np.newaxis, :, :])},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats,\n                    \"time\": time_val\n                }\n            )\n\n            # Add time coordinate attributes indicating it's just a year, not a timestamp\n            ds.time.attrs = {\n                \"long_name\": \"year\",\n                \"standard_name\": \"year\",\n                \"units\": \"year\"\n            }\n        else:\n            # Create a dataset without time dimension (synoptic)\n            ds = xr.Dataset(\n                {\"value\": ([\"lat\", \"lon\"], data)},\n                coords={\n                    \"lon\": lons,\n                    \"lat\": lats\n                }\n            )\n\n        # Add variable attributes\n        ds[\"value\"].attrs = {\n            \"long_name\": variable_name.replace(\"_\", \" \"),\n            \"units\": src.meta.get(\"units\", \"unknown\"),\n            \"resolution\": x_res  # Add resolution to metadata\n        }\n\n        # Add coordinate attributes\n        ds.lon.attrs = {\n            \"long_name\": \"longitude\",\n            \"standard_name\": \"longitude\",\n            \"units\": \"degrees_east\",\n            \"axis\": \"X\",\n            \"resolution\": x_res\n        }\n\n        ds.lat.attrs = {\n            \"long_name\": \"latitude\",\n            \"standard_name\": \"latitude\",\n            \"units\": \"degrees_north\",\n            \"axis\": \"Y\",\n            \"resolution\": y_res\n        }\n\n        # Add global attributes\n        ds.attrs = {\n            \"title\": f\"{variable_name} data\",\n            \"source\": f\"Converted from GeoTIFF: {os.path.basename(input_path)}\",\n            \"created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"Conventions\": \"CF-1.8\",\n            \"history\": f\"Created from {input_path}\",\n            \"pixel_size_degrees\": x_res,\n            \"pixel_registration\": \"center\"\n        }\n\n        # Use default encoding if none provided\n        if encoding is None:\n            encoding = {\n                \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n                \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n                \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5}\n            }\n            if year is not None:\n                # Ensure time is stored as a simple integer with no date units\n                encoding[\"time\"] = {\"dtype\": \"int32\"}\n\n        # Save the dataset to netCDF\n        ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n        return ds\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.main","title":"<code>main()</code>","text":"<p>Main function to run when script is executed directly.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def main():\n    \"\"\"Main function to run when script is executed directly.\"\"\"\n    results = batch_process_all_covariates(skip_existing=False)\n\n    # Print successful conversions\n    print(\"\\nSuccessfully converted covariates:\")\n    for covariate, path in results.items():\n        if path:\n            print(f\"- {covariate}: {path}\")\n\n    # Print failed conversions\n    print(\"\\nFailed conversions:\")\n    for covariate, path in results.items():\n        if not path:\n            print(f\"- {covariate}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable","title":"<code>process_multiyear_variable(covariate_key, output_dir)</code>","text":"<p>Process a multi-year variable and convert to a single netCDF with time dimension.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_multiyear_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_multiyear_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a multi-year variable and convert to a single netCDF with time dimension.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path_template = covariate_dict['path']\n    years = covariate_dict['years']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    if not years:\n        print(f\"No years specified for {covariate_name}. Cannot process as multi-year variable.\")\n        return None\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Update create_multiyear_netcdf to pass resolution to geotiff_to_netcdf\n    # This requires modifying the create_multiyear_netcdf function\n\n    # For now, pass resolution directly to each geotiff_to_netcdf call\n    first_file = True\n    combined_ds = None\n\n    for year in years:\n        input_path = path_template.format(year=year)\n\n        # Check if file exists\n        if not os.path.exists(input_path):\n            print(f\"Warning: File {input_path} does not exist. Skipping.\")\n            continue\n\n        # Create a temporary dataset\n        temp_path = f\"/tmp/{os.path.basename(input_path)}.nc\"\n        temp_ds = geotiff_to_netcdf(\n            input_path=input_path, \n            output_path=temp_path, \n            variable_name=covariate_name, \n            resolution=resolution,\n            year=year\n        )\n\n        # For the first valid file, initialize the combined dataset\n        if first_file:\n            combined_ds = temp_ds\n            first_file = False\n        else:\n            # Concatenate along time dimension\n            combined_ds = xr.concat([combined_ds, temp_ds], dim=\"time\")\n\n        # Remove temporary file\n        os.remove(temp_path)\n\n    if combined_ds is not None:\n        # Sort by time\n        combined_ds = combined_ds.sortby(\"time\")\n\n        # Define encoding for the combined file\n        encoding = {\n            \"value\": {\"zlib\": True, \"complevel\": 5, \"dtype\": \"float32\"},\n            \"lon\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"lat\": {\"dtype\": \"float32\", \"zlib\": True, \"complevel\": 5},\n            \"time\": {\"dtype\": \"int32\"}\n        }\n\n        # Save the combined dataset\n        combined_ds.to_netcdf(output_path, encoding=encoding)\n\n        # Set proper permissions\n        set_file_permissions(output_path)\n\n    return str(output_path) if combined_ds is not None else None\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable","title":"<code>process_synoptic_variable(covariate_key, output_dir)</code>","text":"<p>Process a synoptic (time-invariant) variable and convert to netCDF.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--parameters","title":"Parameters:","text":"<p>covariate_key : str     Key of the covariate in the COVARIATE_DICT</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.process_synoptic_variable--returns","title":"Returns:","text":"<p>str: Path to the output netCDF file</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def process_synoptic_variable(covariate_key, output_dir):\n    \"\"\"\n    Process a synoptic (time-invariant) variable and convert to netCDF.\n\n    Parameters:\n    -----------\n    covariate_key : str\n        Key of the covariate in the COVARIATE_DICT\n\n    Returns:\n    --------\n    str: Path to the output netCDF file\n    \"\"\"\n    # Get dictionary from YAML\n    covariate_dict = parse_yaml_dictionary(covariate_key)\n    covariate_name = covariate_dict.get('covariate_name', covariate_key)\n    path = covariate_dict['path']\n    resolution = covariate_dict.get('covariate_resolution', None)\n\n    # Create output directory if it doesn't exist\n    if isinstance(output_dir, str):\n        output_dir = Path(output_dir)\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output path\n    output_path = output_dir / f\"{covariate_name}.nc\"\n\n    # Convert to netCDF with resolution from YAML\n    geotiff_to_netcdf(\n        input_path=path,\n        output_path=str(output_path),\n        variable_name=covariate_name,\n        resolution=resolution\n    )\n\n    return str(output_path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/01_prep_maps/#idd_forecast_mbp.map_to_admin_2.01_prep_maps.set_file_permissions","title":"<code>set_file_permissions(file_path)</code>","text":"<p>Set 775 permissions (rwxrwxr-x) on the specified file.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/01_prep_maps.py</code> <pre><code>def set_file_permissions(file_path):\n    \"\"\"Set 775 permissions (rwxrwxr-x) on the specified file.\"\"\"\n    try:\n        os.chmod(file_path, 0o775)  # 0o775 is octal for 775 permissions\n    except Exception as e:\n        print(f\"Warning: Could not set permissions on {file_path}: {str(e)}\")\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/02_pixel_main_parallel/","title":"02_pixel_main_parallel","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/03_pixel_hierarchy_parallel/","title":"03_pixel_hierarchy_parallel","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/","title":"pixel_hierarchy","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy","title":"<code>aggregate_climate_to_hierarchy(data: pd.DataFrame, hierarchy: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Create all aggregate climate values for a given hierarchy from most-detailed data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--parameters","title":"Parameters","text":"<p>data     The most-detailed climate data to aggregate. hierarchy     The hierarchy to aggregate the data to.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.aggregate_climate_to_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The climate data with values for all levels of the hierarchy.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def aggregate_climate_to_hierarchy(\n    data: pd.DataFrame, hierarchy: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Create all aggregate climate values for a given hierarchy from most-detailed data.\n\n    Parameters\n    ----------\n    data\n        The most-detailed climate data to aggregate.\n    hierarchy\n        The hierarchy to aggregate the data to.\n\n    Returns\n    -------\n    pd.DataFrame\n        The climate data with values for all levels of the hierarchy.\n    \"\"\"\n    results = data.set_index(\"location_id\").copy()\n\n    # Most detailed locations can be at multiple levels of the hierarchy,\n    # so we loop over all levels from most detailed to global, aggregating\n    # level by level and appending the results to the data.\n    for level in reversed(list(range(1, hierarchy.level.max() + 1))):\n        level_mask = hierarchy.level == level\n        parent_map = hierarchy.loc[level_mask].set_index(\"location_id\").parent_id\n\n        # For every location in the parent map, we need to check if it is the results\n        # For those that are, proceed to aggregate\n        # For those that aren't, check to make sure their parent is in the results. If not, exit with an error\n        absent_parent_map = parent_map.index.difference(results.index)\n        if len(absent_parent_map) &gt; 0:\n            msg = f\"Some parent locations are not in the results: {absent_parent_map}\"\n            # Check to see if the parent of each location id that is missing is in the results\n            parent_of_absent = parent_map.loc[absent_parent_map]\n            unique_parent_ids = parent_of_absent.unique()\n            # Check to see if the unique_parent_ids are in the results\n            missing_parents = unique_parent_ids[~np.isin(unique_parent_ids, results.index)]\n            if len(missing_parents) &gt; 0:\n                msg = f\"Some parent locations are not in the results: {missing_parents}\"\n                raise ValueError(msg)\n\n        present_parent_map = parent_map.loc[parent_map.index.isin(results.index)]\n        # Continue aggregation only on the present locations\n        subset = results.loc[present_parent_map.index]\n        subset[\"parent_id\"] = present_parent_map\n\n        parent_values = (\n            subset.groupby([\"year_id\", \"parent_id\"])[[\"weighted_climate\", \"population\"]]\n            .sum()\n            .reset_index()\n            .rename(columns={\"parent_id\": \"location_id\"})\n            .set_index(\"location_id\")\n        )\n        results = pd.concat([results, parent_values])\n    results = (\n        results.reset_index()\n        .sort_values([\"location_id\", \"year_id\"])\n    )\n    parent_values[\"value\"] = parent_values.weighted_climate / parent_values.population\n    return results\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy","title":"<code>load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame</code>","text":"<p>Load a subset location hierarchy.</p> <p>The subset hierarchy might be equal to the full aggregation hierarchy, but it might also be a subset of the full aggregation hierarchy. These hierarchies are used to provide different views of aggregated climate data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--parameters","title":"Parameters","text":"<p>subset_hierarchy     The administrative hierarchy to load (e.g. \"gbd_2021\")</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.load_subset_hierarchy--returns","title":"Returns","text":"<p>pd.DataFrame     The hierarchy data with parent-child relationships</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def load_subset_hierarchy(subset_hierarchy: str) -&gt; pd.DataFrame:\n    \"\"\"Load a subset location hierarchy.\n\n    The subset hierarchy might be equal to the full aggregation hierarchy,\n    but it might also be a subset of the full aggregation hierarchy.\n    These hierarchies are used to provide different views of aggregated\n    climate data.\n\n    Parameters\n    ----------\n    subset_hierarchy\n        The administrative hierarchy to load (e.g. \"gbd_2021\")\n\n    Returns\n    -------\n    pd.DataFrame\n        The hierarchy data with parent-child relationships\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    allowed_hierarchies = [\"gbd_2021\", \"fhs_2021\", \"lsae_1209\", \"lsae_1285\"]\n    if subset_hierarchy not in allowed_hierarchies:\n        msg = f\"Unknown admin hierarchy: {subset_hierarchy}\"\n        raise ValueError(msg)\n    path = root / \"gbd-inputs\" / f\"hierarchy_{subset_hierarchy}.parquet\"\n    return pd.read_parquet(path)\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy/#idd_forecast_mbp.map_to_admin_2.pixel_hierarchy.post_process","title":"<code>post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame</code>","text":"<p>Rename 000 to {summary_covariate}_per_capita Merge in population Create {summary_covariate}_capita*population -&gt; {summary_covariate}</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_hierarchy.py</code> <pre><code>def post_process(df: pd.DataFrame, pop_df: pd.DataFrame) -&gt; pd.DataFrame: # Fix this for other summary_variable/variable/etc\n    \"\"\"\n    Rename 000 to {summary_covariate}_per_capita\n    Merge in population\n    Create {summary_covariate}_capita*population -&gt; {summary_covariate}\n    \"\"\"\n\n    # Rename 000 to people_flood_days_per_capita\n    df = df.rename(columns={\"000\": f\"{summary_covariate}_per_capita\"})\n\n    # Merge in population\n    full_df = df.merge(\n        pop_df,\n        on=[\"location_id\", \"year_id\"],\n        how=\"left\",\n    )\n    # assert all location_ids and years combinations are present\n    assert df.shape[0] == full_df.shape[0]\n    assert df.location_id.nunique() == full_df.location_id.nunique()\n    assert df.year_id.nunique() == full_df.year_id.nunique()\n\n    # Create {summary_covariate}\n    full_df[summary_covariate] = (\n        full_df[f\"{summary_covariate}_per_capita\"] * full_df[\"population\"]\n    ).astype(np.float32)\n\n    return full_df\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/","title":"pixel_main","text":""},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map","title":"<code>build_bounds_map(raster_template: rt.RasterArray, shape_values: list[tuple[Polygon | MultiPolygon, int]]) -&gt; dict[int, tuple[slice, slice]]</code>","text":"<p>Build a map of location IDs to buffered slices of the raster template.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--parameters","title":"Parameters","text":"<p>raster_template     The raster template to build the bounds map for. shape_values     A list of tuples where the first element is a shapely Polygon or MultiPolygon     in the CRS of the raster template and the second element is the location ID     of the shape.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.build_bounds_map--returns","title":"Returns","text":"<p>dict[int, tuple[slice, slice]]     A dictionary mapping location IDs to a tuple of slices representing the bounds     of the location in the raster template. The slices are buffered by 10 pixels     to ensure that the entire shape is included in the mask.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def build_bounds_map(\n    raster_template: rt.RasterArray,\n    shape_values: list[tuple[Polygon | MultiPolygon, int]],\n) -&gt; dict[int, tuple[slice, slice]]:\n    \"\"\"Build a map of location IDs to buffered slices of the raster template.\n\n    Parameters\n    ----------\n    raster_template\n        The raster template to build the bounds map for.\n    shape_values\n        A list of tuples where the first element is a shapely Polygon or MultiPolygon\n        in the CRS of the raster template and the second element is the location ID\n        of the shape.\n\n    Returns\n    -------\n    dict[int, tuple[slice, slice]]\n        A dictionary mapping location IDs to a tuple of slices representing the bounds\n        of the location in the raster template. The slices are buffered by 10 pixels\n        to ensure that the entire shape is included in the mask.\n    \"\"\"\n    # The tranform maps pixel coordinates to the CRS coordinates.\n    # This mask is the inverse of that transform.\n    to_pixel = ~raster_template.transform\n\n    bounds_map = {}\n    for shp, loc_id in shape_values:\n        xmin, ymin, xmax, ymax = shp.bounds\n        pxmin, pymin = to_pixel * (xmin, ymax)\n        pixel_buffer = 10\n        pxmin = max(0, int(pxmin) - pixel_buffer)\n        pymin = max(0, int(pymin) - pixel_buffer)\n        pxmax, pymax = to_pixel * (xmax, ymin)\n        pxmax = min(raster_template.width, int(pxmax) + pixel_buffer)\n        pymax = min(raster_template.height, int(pymax) + pixel_buffer)\n        bounds_map[loc_id] = (slice(pymin, pymax), slice(pxmin, pxmax))\n\n    return bounds_map\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox","title":"<code>get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon</code>","text":"<p>Get the bounding box of a raster array.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--parameters","title":"Parameters","text":"<p>raster     The raster array to get the bounding box of. crs     The CRS to return the bounding box in. If None, the bounding box     is returned in the CRS of the raster.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.get_bbox--returns","title":"Returns","text":"<p>shapely.Polybon     The bounding box of the raster in the CRS specified by the crs parameter.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def get_bbox(raster: rt.RasterArray, crs: str | None = None) -&gt; shapely.Polygon:\n    \"\"\"Get the bounding box of a raster array.\n\n    Parameters\n    ----------\n    raster\n        The raster array to get the bounding box of.\n    crs\n        The CRS to return the bounding box in. If None, the bounding box\n        is returned in the CRS of the raster.\n\n    Returns\n    -------\n    shapely.Polybon\n        The bounding box of the raster in the CRS specified by the crs parameter.\n    \"\"\"\n    if raster.crs not in MAX_BOUNDS:\n        msg = f\"Unsupported CRS: {raster.crs}\"\n        raise ValueError(msg)\n\n    xmin_clip, xmax_clip, ymin_clip, ymax_clip = MAX_BOUNDS[raster.crs]\n    xmin, xmax, ymin, ymax = raster.bounds\n\n    xmin = np.clip(xmin, xmin_clip, xmax_clip)\n    xmax = np.clip(xmax, xmin_clip, xmax_clip)\n    ymin = np.clip(ymin, ymin_clip, ymax_clip)\n    ymax = np.clip(ymax, ymin_clip, ymax_clip)\n\n    bbox = gpd.GeoSeries([shapely.box(xmin, ymin, xmax, ymax)], crs=raster.crs)\n    out_bbox = bbox.to_crs(crs) if crs is not None else bbox.copy()\n\n    # Check that our transformation didn't do something weird\n    # (e.g. artificially clip the bounds or have the bounds extend over the\n    # antimeridian)\n    check_bbox = out_bbox.to_crs(raster.crs)\n    area_change = (np.abs(bbox.area - check_bbox.area) / bbox.area).iloc[0]\n    tolerance = 1e-6\n    if area_change &gt; tolerance:\n        msg = f\"Area change: {area_change}\"\n        raise ValueError(msg)\n\n    return cast(shapely.Polygon, out_bbox.iloc[0])\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes","title":"<code>load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]) -&gt; gpd.GeoDataFrame</code>","text":"<p>Load shapes for a full aggregation hierarchy within given bounds.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--parameters","title":"Parameters","text":"<p>full_aggregation_hierarchy     The full aggregation hierarchy to load (e.g. \"gbd_2021\") bounds     The bounds to load (xmin, ymin, xmax, ymax)</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.load_raking_shapes--returns","title":"Returns","text":"<p>gpd.GeoDataFrame     The shapes for the given hierarchy and bounds</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def load_raking_shapes(full_aggregation_hierarchy: str, bounds: tuple[float, float, float, float]\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load shapes for a full aggregation hierarchy within given bounds.\n\n    Parameters\n    ----------\n    full_aggregation_hierarchy\n        The full aggregation hierarchy to load (e.g. \"gbd_2021\")\n    bounds\n        The bounds to load (xmin, ymin, xmax, ymax)\n\n    Returns\n    -------\n    gpd.GeoDataFrame\n        The shapes for the given hierarchy and bounds\n    \"\"\"\n    root = Path(\"/mnt/team/rapidresponse/pub/population-model/admin-inputs/raking\")\n    if full_aggregation_hierarchy in [\"gbd_2021\", \"gbd_2023\"]:\n        shape_path = (\n            root/ f\"shapes_{full_aggregation_hierarchy}.parquet\"\n        )\n        gdf = gpd.read_parquet(shape_path, bbox=bounds)\n\n        # We're using population data here instead of a hierarchy because\n        # The populations include extra locations we've supplemented that aren't\n        # modeled in GBD (e.g. locations with zero population or places that\n        # GBD uses population scalars from WPP to model)\n        pop_path = (\n            root / f\"population_{full_aggregation_hierarchy}.parquet\"\n        )\n        pop = pd.read_parquet(pop_path)\n\n        keep_cols = [\"location_id\", \"location_name\", \"most_detailed\", \"parent_id\"]\n        keep_mask = (\n            (pop.year_id == pop.year_id.max())  # Year doesn't matter\n            &amp; (pop.most_detailed == 1)\n        )\n        out = gdf.merge(pop.loc[keep_mask, keep_cols], on=\"location_id\", how=\"left\")\n    elif full_aggregation_hierarchy in [\"lsae_1209\", \"lsae_1285\"]:\n        # This is only a2 geoms, so already most detailed\n        shape_path = (\n            root\n            / \"gbd-inputs\"\n            / f\"shapes_{full_aggregation_hierarchy}_a2.parquet\"\n        )\n        out = gpd.read_parquet(shape_path, bbox=bounds)\n    else:\n        msg = f\"Unknown pixel hierarchy: {full_aggregation_hierarchy}\"\n        raise ValueError(msg)\n    return out\n</code></pre>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster","title":"<code>to_raster(ds: xr.DataArray, no_data_value: float | int, lat_col: str = 'lat', lon_col: str = 'lon', crs: str = 'EPSG:4326') -&gt; rt.RasterArray</code>","text":"<p>Convert an xarray DataArray to a RasterArray.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--parameters","title":"Parameters","text":"<p>ds     The xarray DataArray to convert. no_data_value     The value to use for missing data. This should be consistent with the dtype of the data. lat_col     The name of the latitude coordinate in the dataset. lon_col     The name of the longitude coordinate in the dataset. crs     The coordinate reference system of the data.</p>"},{"location":"reference/idd_forecast_mbp/map_to_admin_2/pixel_main/#idd_forecast_mbp.map_to_admin_2.pixel_main.to_raster--returns","title":"Returns","text":"<p>rt.RasterArray     The RasterArray representation of the input data.</p> Source code in <code>src/idd_forecast_mbp/map_to_admin_2/pixel_main.py</code> <pre><code>def to_raster(\n    ds: xr.DataArray,\n    no_data_value: float | int,\n    lat_col: str = \"lat\",\n    lon_col: str = \"lon\",\n    crs: str = \"EPSG:4326\",\n) -&gt; rt.RasterArray:\n    \"\"\"Convert an xarray DataArray to a RasterArray.\n\n    Parameters\n    ----------\n    ds\n        The xarray DataArray to convert.\n    no_data_value\n        The value to use for missing data. This should be consistent with the dtype of the data.\n    lat_col\n        The name of the latitude coordinate in the dataset.\n    lon_col\n        The name of the longitude coordinate in the dataset.\n    crs\n        The coordinate reference system of the data.\n\n    Returns\n    -------\n    rt.RasterArray\n        The RasterArray representation of the input data.\n    \"\"\"\n    lat, lon = ds[lat_col].data, ds[lon_col].data\n\n    dlat = (lat[1:] - lat[:-1]).mean()\n    dlon = (lon[1:] - lon[:-1]).mean()\n\n    transform = Affine(\n        a=dlon,\n        b=0.0,\n        c=lon[0],\n        d=0.0,\n        e=-dlat,\n        f=lat[-1],\n    )\n    return rt.RasterArray(\n        data=ds.data[::-1],\n        transform=transform,\n        crs=crs,\n        no_data_value=no_data_value,\n    )\n</code></pre>"}]}