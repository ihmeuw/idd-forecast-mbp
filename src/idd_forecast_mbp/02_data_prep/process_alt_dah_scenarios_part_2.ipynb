{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c4ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr # type: ignore\n",
    "from pathlib import Path\n",
    "import numpy as np # type: ignore\n",
    "from typing import cast\n",
    "import numpy.typing as npt # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from typing import Literal, NamedTuple\n",
    "import itertools\n",
    "from rra_tools.shell_tools import mkdir # type: ignore\n",
    "from idd_forecast_mbp import constants as rfc\n",
    "from idd_forecast_mbp.helper_functions import merge_dataframes, read_income_paths, read_urban_paths, level_filter\n",
    "from idd_forecast_mbp.parquet_functions import read_parquet_with_integer_ids, write_parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f59ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = rfc.draws\n",
    "ssp_scenarios = rfc.ssp_scenarios\n",
    "# Hierarchy\n",
    "hierarchy = \"lsae_1209\"\n",
    "PROCESSED_DATA_PATH = rfc.MODEL_ROOT / \"02-processed_data\"\n",
    "MODELING_DATA_PATH = rfc.MODEL_ROOT / \"03-modeling_data\"\n",
    "FORECASTING_DATA_PATH = rfc.MODEL_ROOT / \"04-forecasting_data\"\n",
    "\n",
    "\n",
    "cause = \"malaria\"\n",
    "\n",
    "# New DAH data\n",
    "new_dah_scenarios = {\n",
    "    'reference': {\n",
    "        'name': 'reference',\n",
    "        'path': f'{PROCESSED_DATA_PATH}/dah_reference_df.parquet'\n",
    "    },\n",
    "    'better': {\n",
    "        'name': 'better',\n",
    "        'path': f'{PROCESSED_DATA_PATH}/dah_better_df.parquet'\n",
    "    },\n",
    "    'worse': {\n",
    "        'name': 'worse',\n",
    "        'path': f'{PROCESSED_DATA_PATH}/dah_worse_df.parquet'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "cause_map = rfc.cause_map\n",
    "reference_age_group_id = cause_map[cause]['reference_age_group_id']\n",
    "reference_sex_id = cause_map[cause]['reference_sex_id']\n",
    "\n",
    "base_dah_scenario_df_path_template = \"{FORECASTING_DATA_PATH}/malaria_forecast_ssp_scenario_{ssp_scenario}_dah_scenario_Baseline_draw_{draw}.parquet\"\n",
    "dah_scenario_df_path_template = \"{FORECASTING_DATA_PATH}/malaria_forecast_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario_name}_draw_{draw}.parquet\"\n",
    "\n",
    "columns_to_keep = ['location_id', 'year_id', 'people_flood_days_per_capita', \n",
    "    'gdppc_mean', 'log_gdppc_mean', \n",
    "    'logit_malaria_pfpr',\n",
    "    'aa_malaria_mort_rate', 'aa_malaria_inc_rate',\n",
    "    'base_malaria_mort_rate', 'base_malaria_inc_rate',\n",
    "    'log_aa_malaria_mort_rate', 'log_aa_malaria_inc_rate',\n",
    "    'log_base_malaria_mort_rate', 'log_base_malaria_inc_rate', \n",
    "    'malaria_suitability', 'year_to_rake_to', 'A0_af']\n",
    "\n",
    "dah_columns_to_keep = ['location_id', 'year_id', 'mal_DAH_total_per_capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SSP scenario: ssp126, Draw: 000\n",
      "Processing DAH scenario: reference\n",
      "✅ Metadata validation passed for /mnt/team/idd/pub/forecast-mbp/04-forecasting_data/malaria_forecast_ssp_scenario_ssp126_dah_scenario_reference_draw_000.parquet\n",
      "Processing DAH scenario: better\n",
      "✅ Metadata validation passed for /mnt/team/idd/pub/forecast-mbp/04-forecasting_data/malaria_forecast_ssp_scenario_ssp126_dah_scenario_better_draw_000.parquet\n",
      "Processing DAH scenario: worse\n",
      "✅ Metadata validation passed for /mnt/team/idd/pub/forecast-mbp/04-forecasting_data/malaria_forecast_ssp_scenario_ssp126_dah_scenario_worse_draw_000.parquet\n",
      "Processing SSP scenario: ssp126, Draw: 001\n",
      "Processing DAH scenario: reference\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m dah_df = read_parquet_with_integer_ids(dah_scenario[\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     19\u001b[39m     columns=dah_columns_to_keep)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Merge with the existing DAH scenario data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m dah_scenario_df = \u001b[43mbase_dah_scenario_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdah_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocation_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myear_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Add the new DAH column\u001b[39;00m\n\u001b[32m     25\u001b[39m dah_scenario_df[\u001b[33m'\u001b[39m\u001b[33mmal_DAH_total_per_capita\u001b[39m\u001b[33m'\u001b[39m] = dah_scenario_df[\u001b[33m'\u001b[39m\u001b[33mmal_DAH_total_per_capita\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/frame.py:10832\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10813\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10814\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10828\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m ) -> DataFrame:\n\u001b[32m  10830\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10833\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:886\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    888\u001b[39m result = \u001b[38;5;28mself\u001b[39m._reindex_and_concat(\n\u001b[32m    889\u001b[39m     join_index, left_indexer, right_indexer, copy=copy\n\u001b[32m    890\u001b[39m )\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1151\u001b[39m, in \u001b[36m_MergeOperation._get_join_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m     join_index, right_indexer, left_indexer = _left_join_on_index(\n\u001b[32m   1148\u001b[39m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m.right_join_keys, sort=\u001b[38;5;28mself\u001b[39m.sort\n\u001b[32m   1149\u001b[39m     )\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     (left_indexer, right_indexer) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.right_index:\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.left) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1125\u001b[39m, in \u001b[36m_MergeOperation._get_join_indexers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.how != \u001b[33m\"\u001b[39m\u001b[33masof\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhow\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1759\u001b[39m, in \u001b[36mget_join_indexers\u001b[39m\u001b[34m(left_keys, right_keys, sort, how)\u001b[39m\n\u001b[32m   1757\u001b[39m     _, lidx, ridx = left.join(right, how=how, return_indexers=\u001b[38;5;28;01mTrue\u001b[39;00m, sort=sort)\n\u001b[32m   1758\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1759\u001b[39m     lidx, ridx = \u001b[43mget_join_indexers_non_unique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\n\u001b[32m   1761\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[32m   1764\u001b[39m     lidx = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1793\u001b[39m, in \u001b[36mget_join_indexers_non_unique\u001b[39m\u001b[34m(left, right, sort, how)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_join_indexers_non_unique\u001b[39m(\n\u001b[32m   1771\u001b[39m     left: ArrayLike,\n\u001b[32m   1772\u001b[39m     right: ArrayLike,\n\u001b[32m   1773\u001b[39m     sort: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1774\u001b[39m     how: JoinHow = \u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1775\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[npt.NDArray[np.intp], npt.NDArray[np.intp]]:\n\u001b[32m   1776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1777\u001b[39m \u001b[33;03m    Get join indexers for left and right.\u001b[39;00m\n\u001b[32m   1778\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1791\u001b[39m \u001b[33;03m        Indexer into right.\u001b[39;00m\n\u001b[32m   1792\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     lkey, rkey, count = \u001b[43m_factorize_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1795\u001b[39m         lidx, ridx = libjoin.left_outer_join(lkey, rkey, count, sort=sort)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/forecast-mbp/lib/python3.12/site-packages/pandas/core/reshape/merge.py:2562\u001b[39m, in \u001b[36m_factorize_keys\u001b[39m\u001b[34m(lk, rk, sort)\u001b[39m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2558\u001b[39m     \u001b[38;5;66;03m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;66;03m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[32m   2560\u001b[39m     \u001b[38;5;66;03m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[32m   2561\u001b[39m     llab = rizer.factorize(lk)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2562\u001b[39m     rlab = \u001b[43mrizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m llab.dtype == np.dtype(np.intp), llab.dtype\n\u001b[32m   2564\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m rlab.dtype == np.dtype(np.intp), rlab.dtype\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for ssp_scenario in ssp_scenarios:\n",
    "    for draw in draws:\n",
    "        print(f\"Processing SSP scenario: {ssp_scenario}, Draw: {draw}\")\n",
    "\n",
    "        base_dah_scenario_df_path = base_dah_scenario_df_path_template.format(\n",
    "            FORECASTING_DATA_PATH=FORECASTING_DATA_PATH,\n",
    "            ssp_scenario=ssp_scenario,\n",
    "            draw=draw\n",
    "        )\n",
    "        base_dah_scenario_df = read_parquet_with_integer_ids(base_dah_scenario_df_path,\n",
    "            columns=columns_to_keep\n",
    "        )\n",
    "        \n",
    "        for dah_scenario_name, dah_scenario in new_dah_scenarios.items():\n",
    "            print(f\"Processing DAH scenario: {dah_scenario_name}\")\n",
    "            \n",
    "            # Read the new DAH scenario data\n",
    "            dah_df = read_parquet_with_integer_ids(dah_scenario['path'],\n",
    "                columns=dah_columns_to_keep)\n",
    "            \n",
    "            # Merge with the existing DAH scenario data\n",
    "            dah_scenario_df = base_dah_scenario_df.merge(dah_df, on=['location_id', 'year_id'], how='left')\n",
    "            \n",
    "            # Add the new DAH column\n",
    "            dah_scenario_df['mal_DAH_total_per_capita'] = dah_scenario_df['mal_DAH_total_per_capita'].fillna(0)\n",
    "            \n",
    "            # Write the output to a new parquet file\n",
    "            dah_scenario_df_path = dah_scenario_df_path_template.format(\n",
    "                FORECASTING_DATA_PATH=FORECASTING_DATA_PATH,\n",
    "                ssp_scenario=ssp_scenario,\n",
    "                dah_scenario_name=dah_scenario_name,\n",
    "                draw=draw\n",
    "            )\n",
    "\n",
    "            write_parquet(dah_scenario_df, dah_scenario_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed38d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LSAE 1209 variable path\n",
    "VARIABLE_DATA_PATH = f\"{PROCESSED_DATA_PATH}/{hierarchy}\"\n",
    "# CLIMATE 1209 variable path\n",
    "CLIMATE_DATA_PATH = f\"/mnt/team/rapidresponse/pub/climate-aggregates/2025_03_20/results/{hierarchy}\"\n",
    "\n",
    "# Malaria modeling dataframes\n",
    "aa_full_cause_df_path_template = f'{PROCESSED_DATA_PATH}/aa_full_{cause}_df.parquet'\n",
    "as_full_cause_df_path_template = f'{PROCESSED_DATA_PATH}/as_full_{cause}_df.parquet'\n",
    "\n",
    "# Climate variables\n",
    "cc_sensitive_paths = {\n",
    "    \"total_precipitation\":      \"{CLIMATE_DATA_PATH}/total_precipitation_{ssp_scenario}.parquet\",\n",
    "    \"relative_humidity\":        \"{CLIMATE_DATA_PATH}/relative_humidity_{ssp_scenario}.parquet\",\n",
    "    \"malaria_suitability\":      \"{CLIMATE_DATA_PATH}/malaria_suitability_{ssp_scenario}.parquet\",\n",
    "}\n",
    "\n",
    "####################################\n",
    "\n",
    "def generate_dah_scenarios(\n",
    "    baseline_df,\n",
    "    ssp_scenario,\n",
    "    year_start = 2000,\n",
    "    reference_year=2023,\n",
    "    modification_start_year=2026,\n",
    "    dah_scenario_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate DAH funding scenarios for malaria forecasting.\n",
    "    \n",
    "    This function creates four scenarios:\n",
    "    1. Baseline: Original DAH projections\n",
    "    2. Constant: Holds DAH at reference_year levels for future years\n",
    "    3. Increasing: Progressively increases DAH (1.2, 1.4, 1.6, 1.8, 2.0)\n",
    "    4. Decreasing: Progressively decreases DAH (0.8, 0.6, 0.4, 0.2, 0.0)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ssp_scenarios : list\n",
    "        List of SSP climate scenario names\n",
    "    draw : str\n",
    "        Draw identifier\n",
    "    forecasting_data_path : str\n",
    "        Path to forecasting data\n",
    "    malaria_forecasting_df_path : str\n",
    "        Template for malaria forecasting data path with format parameters\n",
    "    hierarchy_df : pandas.DataFrame\n",
    "        Hierarchy data with location and super region information\n",
    "    year_start : int\n",
    "        Starting year for filtering data\n",
    "    reference_year : int, optional\n",
    "        Year to use as reference for constant scenario, default 2023\n",
    "    modification_start_year : int, optional\n",
    "        Year to start the increasing/decreasing changes, default 2026\n",
    "    scenario_names : list, optional\n",
    "        Custom names for the scenarios, default ['Baseline', 'Constant', 'Increasing', 'Decreasing']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (dah_scenarios, dah_scenario_names) - List of scenario DataFrame lists and scenario names\n",
    "    \"\"\"\n",
    "    # Set default scenario names if not provided\n",
    "    if dah_scenario_names is None:\n",
    "        dah_scenario_names = ['Baseline', 'Constant', 'Increasing', 'Decreasing']\n",
    "    \n",
    "    # Define modification schedules\n",
    "    increasing_factors = {\n",
    "        modification_start_year: 1.2,\n",
    "        modification_start_year + 1: 1.4,\n",
    "        modification_start_year + 2: 1.6,\n",
    "        modification_start_year + 3: 1.8,\n",
    "        modification_start_year + 4: 2.0\n",
    "    }\n",
    "    \n",
    "    decreasing_factors = {\n",
    "        modification_start_year: 0.8,\n",
    "        modification_start_year + 1: 0.6,\n",
    "        modification_start_year + 2: 0.4,\n",
    "        modification_start_year + 3: 0.2,\n",
    "        modification_start_year + 4: 0.0\n",
    "    }\n",
    "\n",
    "    # Process baseline data\n",
    "    baseline_df = baseline_df.copy() \n",
    "    baseline_df = baseline_df[baseline_df['year_id'] >= year_start]\n",
    "    baseline_df[\"A0_location_id\"] = baseline_df[\"A0_location_id\"].astype(int)\n",
    "    baseline_df['A0_af'] = 'A0_' + baseline_df['A0_location_id'].astype(str)\n",
    "      \n",
    "    baseline_df['ssp_scenario'] = ssp_scenario\n",
    "    baseline_df['dah_scenario'] = 'Baseline'\n",
    "    \n",
    "    # Create Scenario 1: Constant DAH at reference year level\n",
    "    print(f\"  Scenario 1: Constant DAH\")\n",
    "    scenario_1_df = baseline_df.copy()\n",
    "    scenario_1_df['mal_DAH_total'] = scenario_1_df['mal_DAH_total_per_capita'] * scenario_1_df['aa_population']\n",
    "    \n",
    "    # Get reference year values and merge\n",
    "    values_ref_year = scenario_1_df[scenario_1_df['year_id'] == reference_year][['location_id', 'mal_DAH_total']]\n",
    "    scenario_1_df = scenario_1_df.merge(values_ref_year, on='location_id', suffixes=('', f'_{reference_year}'))\n",
    "    \n",
    "    # Replace future values with reference year values\n",
    "    mask = scenario_1_df['year_id'] >= reference_year + 1\n",
    "    scenario_1_df.loc[mask, 'mal_DAH_total'] = scenario_1_df.loc[mask, f'mal_DAH_total_{reference_year}']\n",
    "    \n",
    "    # Drop temporary column\n",
    "    scenario_1_df = scenario_1_df.drop(columns=f'mal_DAH_total_{reference_year}')\n",
    "    \n",
    "    # Recalculate per-capita values\n",
    "    scenario_1_df['mal_DAH_total_per_capita'] = scenario_1_df['mal_DAH_total'] / scenario_1_df['aa_population']\n",
    "\n",
    "    scenario_1_df['log_mal_DAH_total_per_capita'] = np.log(scenario_1_df['mal_DAH_total_per_capita'] + 1e-6)\n",
    "    scenario_1_df['ssp_scenario'] = ssp_scenario\n",
    "    scenario_1_df['dah_scenario'] = 'Constant'\n",
    "    \n",
    "    # Create Scenario 2: Increasing DAH\n",
    "    print(f\"  Scenario 2: Increasing DAH\")\n",
    "    scenario_2_df = baseline_df.copy()\n",
    "    \n",
    "    # Apply increasing factors to specific years\n",
    "    for year, factor in increasing_factors.items():\n",
    "        mask = scenario_2_df['year_id'] == year\n",
    "        scenario_2_df.loc[mask, 'mal_DAH_total_per_capita'] *= factor\n",
    "        scenario_2_df.loc[mask, 'mal_DAH_total'] *= factor\n",
    "    \n",
    "    # Apply maximum factor to all later years\n",
    "    max_factor = max(increasing_factors.values())\n",
    "    max_year = max(increasing_factors.keys())\n",
    "    mask = scenario_2_df['year_id'] > max_year\n",
    "    scenario_2_df.loc[mask, 'mal_DAH_total_per_capita'] *= max_factor\n",
    "    scenario_2_df.loc[mask, 'mal_DAH_total'] = scenario_2_df.loc[mask, 'mal_DAH_total_per_capita'] * scenario_2_df.loc[mask, 'aa_population']\n",
    "    \n",
    "    # Recalculate derived values\n",
    "    scenario_2_df['log_mal_DAH_total_per_capita'] = np.log(scenario_2_df['mal_DAH_total_per_capita'] + 1e-6)\n",
    "    scenario_2_df['ssp_scenario'] = ssp_scenario\n",
    "    scenario_2_df['dah_scenario'] = 'Increasing'\n",
    "    \n",
    "    # Create Scenario 3: Decreasing DAH\n",
    "    print(f\"  Scenario 3: Decreasing DAH\")\n",
    "    scenario_3_df = baseline_df.copy()\n",
    "    \n",
    "    # Apply decreasing factors to specific years\n",
    "    for year, factor in decreasing_factors.items():\n",
    "        mask = scenario_3_df['year_id'] == year\n",
    "        scenario_3_df.loc[mask, 'mal_DAH_total_per_capita'] *= factor\n",
    "        scenario_3_df.loc[mask, 'mal_DAH_total'] *= factor\n",
    "    \n",
    "    # Apply minimum factor to all later years\n",
    "    min_factor = min(decreasing_factors.values())\n",
    "    max_year = max(decreasing_factors.keys())\n",
    "    mask = scenario_3_df['year_id'] > max_year\n",
    "    scenario_3_df.loc[mask, 'mal_DAH_total_per_capita'] = min_factor  # Use 0 or min_factor\n",
    "    scenario_3_df.loc[mask, 'mal_DAH_total'] = min_factor * scenario_3_df.loc[mask, 'aa_population']\n",
    "    \n",
    "    # Recalculate derived values\n",
    "    scenario_3_df['log_mal_DAH_total_per_capita'] = np.log(scenario_3_df['mal_DAH_total_per_capita'] + 1e-6)\n",
    "    scenario_3_df['ssp_scenario'] = ssp_scenario\n",
    "    scenario_3_df['dah_scenario'] = 'Decreasing'\n",
    "    \n",
    "    # Group all scenarios\n",
    "    dah_scenarios = [baseline_df, scenario_1_df, scenario_2_df, scenario_3_df]    \n",
    "    return dah_scenarios, dah_scenario_names\n",
    "\n",
    "aa_malaria_df = read_parquet_with_integer_ids(aa_full_cause_df_path_template,\n",
    "    filters=[level_filter(hierarchy_df, start_level = 3, end_level = 5)])\n",
    "\n",
    "aa_malaria_df = aa_malaria_df.merge(hierarchy_df[['location_id', 'A0_location_id', 'level']],\n",
    "    how=\"left\",\n",
    "    on=\"location_id\")\n",
    "\n",
    "aa_A0_malaria_df = aa_malaria_df[(aa_malaria_df[\"location_id\"] == aa_malaria_df[\"A0_location_id\"]) & (aa_malaria_df[\"year_id\"] == 2022)].copy()\n",
    "aa_A0_malaria_df = aa_A0_malaria_df[aa_A0_malaria_df['malaria_mort_count'] > malaria_mortality_threshold].copy()\n",
    "A0_malaria_ids = aa_A0_malaria_df['A0_location_id'].unique()\n",
    "\n",
    "aa_malaria_df = aa_malaria_df[aa_malaria_df['A0_location_id'].isin(A0_malaria_ids)].copy()\n",
    "aa_malaria_df = aa_malaria_df[\n",
    "    (aa_malaria_df[\"malaria_pfpr\"] > 0) &\n",
    "    (aa_malaria_df[\"malaria_mort_count\"] > 0) &\n",
    "    (aa_malaria_df[\"malaria_inc_count\"] >= 0) &\n",
    "    (aa_malaria_df[\"level\"] == 5)].copy()\n",
    "\n",
    "aa_malaria_ids = aa_malaria_df['location_id'].unique()\n",
    "aa_malaria_filter = ('location_id', 'in', aa_malaria_ids.tolist())\n",
    "\n",
    "reference_age_group_filter = ('age_group_id', '==', reference_age_group_id)\n",
    "reference_sex_filter = ('sex_id', '==', reference_sex_id)\n",
    "as_base_malaria_df = read_parquet_with_integer_ids(as_full_cause_df_path_template,\n",
    "    filters=[reference_age_group_filter, reference_sex_filter, aa_malaria_filter]).drop(columns=['age_group_id', 'sex_id', 'aa_population'])\n",
    "\n",
    "as_base_malaria_df = as_base_malaria_df.rename(columns=lambda x: f\"base_{x}\" if (x.startswith('malaria_') or x.startswith('pop_')) else x)\n",
    "as_base_malaria_df = as_base_malaria_df.merge(\n",
    "    aa_malaria_df[aa_merge_variables + ['malaria_pfpr']],\n",
    "    how=\"left\",\n",
    "    on=aa_merge_variables)\n",
    "\n",
    "covariates_to_log_transform = [col for col in as_base_malaria_df.columns if 'rate' in col]\n",
    "for col in covariates_to_log_transform:\n",
    "    # Create a new column with the log transformed value\n",
    "    as_base_malaria_df[f\"log_{col}\"] = np.log(as_base_malaria_df[col])\n",
    "\n",
    "as_base_malaria_df[f\"logit_malaria_pfpr\"] = np.log(0.999 * as_base_malaria_df[\"malaria_pfpr\"] / (1 - 0.999 * as_base_malaria_df[\"malaria_pfpr\"]))\n",
    "\n",
    "forecast_by_draw_df = read_parquet_with_integer_ids(forecast_non_draw_df_path,\n",
    "    filters=[aa_malaria_filter])\n",
    "\n",
    "# Add the draw column\n",
    "forecast_by_draw_df[\"draw\"] = draw\n",
    "forecast_by_draw_df = forecast_by_draw_df.rename(columns={\n",
    "    'population': 'aa_population'})\n",
    "\n",
    "forecast_by_draw_df = forecast_by_draw_df.merge(as_base_malaria_df, \n",
    "    how='left',\n",
    "    on=['location_id','year_id'])\n",
    "\n",
    "for key, path_template in cc_sensitive_paths.items():\n",
    "    # Replace {ssp_scenario} in the path with the current ssp_scenario\n",
    "    path = path_template.format(CLIMATE_DATA_PATH=CLIMATE_DATA_PATH, ssp_scenario=ssp_scenario)\n",
    "    # Read the parquet file\n",
    "    columns_to_read = [\"location_id\", \"year_id\", draw]\n",
    "    df = read_parquet_with_integer_ids(path, columns=columns_to_read,\n",
    "        filters=[aa_malaria_filter])\n",
    "    df = df.rename(columns={draw: key})\n",
    "    # Merge the file with forecast_by_draw_df\n",
    "    forecast_by_draw_df = pd.merge(forecast_by_draw_df, df, on=[\"location_id\", \"year_id\"], how=\"left\")\n",
    "\n",
    "covariates_to_log_transform = [\n",
    "    \"mal_DAH_total_per_capita\",\n",
    "    \"gdppc_mean\",\n",
    "]\n",
    "\n",
    "for col in covariates_to_log_transform:\n",
    "    # Create a new column with the log transformed value\n",
    "    forecast_by_draw_df[f\"log_{col}\"] = np.log(forecast_by_draw_df[col] + 1e-6)\n",
    "\n",
    "pakistan_id = 165\n",
    "pakistan_children_ids = hierarchy_df[hierarchy_df['parent_id'] == pakistan_id]['location_id'].tolist()\n",
    "pakistan_grandchildren_ids = hierarchy_df[hierarchy_df['parent_id'].isin(pakistan_children_ids)]['location_id'].tolist()\n",
    "# Combine all Pakistan-related location IDs\n",
    "all_pakistan_locations = [pakistan_id] + pakistan_children_ids + pakistan_grandchildren_ids\n",
    "\n",
    "forecast_by_draw_df['year_to_rake_to'] = 2022\n",
    "forecast_by_draw_df.loc[forecast_by_draw_df['location_id'].isin(all_pakistan_locations), 'year_to_rake_to'] = 2021\n",
    "\n",
    "dah_scenarios, dah_scenario_names = generate_dah_scenarios(\n",
    "    baseline_df=forecast_by_draw_df,\n",
    "    ssp_scenario=ssp_scenario\n",
    ")\n",
    "\n",
    "for dah_scenario_df, dah_scenario_name in zip(dah_scenarios, dah_scenario_names):\n",
    "    # Write each scenario to a parquet file\n",
    "    scenario_df = dah_scenario_df.copy()\n",
    "\n",
    "    dah_scenario_df_path = dah_scenario_df_path_template.format(\n",
    "        FORECASTING_DATA_PATH=FORECASTING_DATA_PATH,\n",
    "        cause=cause,\n",
    "        ssp_scenario=ssp_scenario,\n",
    "        dah_scenario_name=dah_scenario_name,\n",
    "        draw=draw\n",
    "    )\n",
    "    write_parquet(dah_scenario_df, dah_scenario_df_path)\n",
    "\n",
    "# Write the malaria_stage_2_modeling_df to a parquet file\n",
    "forecast_by_draw_df_path = forecast_by_draw_df_path_template.format(\n",
    "    FORECASTING_DATA_PATH=FORECASTING_DATA_PATH,\n",
    "    cause=cause,\n",
    "    ssp_scenario=ssp_scenario,\n",
    "    draw=draw\n",
    ")\n",
    "write_parquet(forecast_by_draw_df, forecast_by_draw_df_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-mbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
