{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr # type: ignore\n",
    "from pathlib import Path\n",
    "import numpy as np # type: ignore\n",
    "from affine import Affine # type: ignore\n",
    "from typing import cast\n",
    "import numpy.typing as npt # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from typing import Literal, NamedTuple\n",
    "import itertools\n",
    "from rra_tools.shell_tools import mkdir # type: ignore\n",
    "from idd_forecast_mbp import constants as rfc\n",
    "from idd_forecast_mbp.helper_functions import check_folders_for_files\n",
    "from idd_forecast_mbp.hd5_functions import write_hdf\n",
    "from idd_forecast_mbp.parquet_functions import read_parquet_with_integer_ids\n",
    "from idd_forecast_mbp.xarray_functions import convert_with_preset, write_netcdf, read_netcdf_with_integer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_date = '2025_08_11'\n",
    "metric = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = rfc.MODEL_ROOT / \"02-processed_data\"\n",
    "MODELING_DATA_PATH = rfc.MODEL_ROOT / \"03-modeling_data\"\n",
    "FORECASTING_DATA_PATH = rfc.MODEL_ROOT / \"04-forecasting_data\"\n",
    "UPLOAD_DATA_PATH = rfc.MODEL_ROOT / \"05-upload_data\"\n",
    "# FINAL_UPLOAD_DATA_PATH = '/mnt/team/integrated_analytics/pub/goalkeepers/goalkeepers_2025/'\n",
    "FINAL_UPLOAD_DATA_PATH = UPLOAD_DATA_PATH\n",
    "FHS_DATA_PATH = f\"{PROCESSED_DATA_PATH}/age_specific_fhs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe636c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_draws = rfc.draws\n",
    "measure_map = rfc.measure_map\n",
    "metric_map = rfc.metric_map\n",
    "cause_map = rfc.cause_map\n",
    "ssp_scenarios = rfc.ssp_scenarios\n",
    "scenario = ssp_scenarios[ssp_scenario][\"dhs_scenario\"] #  is the DHS scenario name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2156f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if cause == \"malaria\":\n",
    "    if hold_variable == 'None':\n",
    "        processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}_draw_{draw}_with_predictions.nc\"\n",
    "        as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}\"\n",
    "        aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}\"\n",
    "    else:\n",
    "        processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}_draw_{draw}_with_predictions_hold_{hold_variable}.nc\"\n",
    "        as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}_hold_variable_{hold_variable}\"\n",
    "        aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_dah_scenario_{dah_scenario}_hold_variable_{hold_variable}\"\n",
    "    \n",
    "else:\n",
    "    if hold_variable == 'None':\n",
    "        if vaccinate == 'None':\n",
    "            processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_draw_{draw}_with_predictions.nc\"\n",
    "            as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}\"\n",
    "            aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}\"\n",
    "        else:\n",
    "            processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_no_vaccinate_draw_{draw}_with_predictions.nc\"\n",
    "            as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_no_vaccinate\"\n",
    "            aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_no_vaccinate\"\n",
    "    else:\n",
    "        if vaccinate == 'None':\n",
    "            processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_draw_{draw}_with_predictions_hold_{hold_variable}.nc\"\n",
    "            as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_hold_variable_{hold_variable}\"\n",
    "            aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_hold_variable_{hold_variable}\"\n",
    "        else:\n",
    "            processed_forecast_ds_path_template = \"{UPLOAD_DATA_PATH}/full_as_{cause}_measure_{measure}_ssp_scenario_{ssp_scenario}_no_vaccinate_draw_{draw}_with_predictions_hold_{hold_variable}.nc\"\n",
    "            as_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/as_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_no_vaccinate_hold_variable_{hold_variable}\"\n",
    "            aa_upload_folder_path = f\"{FINAL_UPLOAD_DATA_PATH}/upload_folders/{run_date}/aa_cause_{cause}_measure_{measure}_metric_{metric}_ssp_scenario_{ssp_scenario}_no_vaccinate_hold_variable_{hold_variable}\"\n",
    "    \n",
    "as_upload_draws_file_path = f\"{as_upload_folder_path}/draws.nc\"\n",
    "as_upload_mean_file_path = f\"{as_upload_folder_path}/mean.nc\"\n",
    "aa_upload_draws_file_path = f\"{aa_upload_folder_path}/draws.nc\"\n",
    "aa_upload_mean_file_path = f\"{aa_upload_folder_path}/mean.nc\"\n",
    "\n",
    "folders_and_files = {\n",
    "    as_upload_folder_path: [as_upload_draws_file_path, as_upload_mean_file_path],\n",
    "    aa_upload_folder_path: [aa_upload_draws_file_path, aa_upload_mean_file_path]\n",
    "}\n",
    "\n",
    "# Check all folders and their files at once\n",
    "folder_results = check_folders_for_files(folders_and_files, delete_existing=delete_existing)\n",
    "no_need_to_run = all(folder_results.values())\n",
    "\n",
    "if no_need_to_run:\n",
    "    print(\"All required files are already present and you didn't say to delete. No need to run the script.\")\n",
    "    exit(0)\n",
    "else:\n",
    "    print(\"Some files or folders were missing or needed to be deleted. Proceeding with the script.\")\n",
    "\n",
    "age_metadata_path = f\"{FHS_DATA_PATH}/age_metadata.parquet\"\n",
    "\n",
    "# Hierarchy path\n",
    "hierarchy_df_path = f'{PROCESSED_DATA_PATH}/full_hierarchy_lsae_1209.parquet'\n",
    "hierarchy_df = read_parquet_with_integer_ids(hierarchy_df_path)\n",
    "\n",
    "as_full_population_df_path = f\"{PROCESSED_DATA_PATH}/as_2023_full_population.parquet\"\n",
    "aa_full_population_df_path = f\"{PROCESSED_DATA_PATH}/aa_2023_full_population.parquet\"\n",
    "as_merge_variables = rfc.as_merge_variables\n",
    "aa_merge_variables = rfc.aa_merge_variables\n",
    "\n",
    "swap_location_ids = [60908, 95069, 94364]\n",
    "\n",
    "all_location_ids = hierarchy_df[\"location_id\"].unique().tolist()\n",
    "\n",
    "year_ids = range(2022, 2101)\n",
    "# Make filters based on hierarchy_df\n",
    "all_location_filter = ('location_id', 'in', all_location_ids)\n",
    "\n",
    "# Use all_location_ids for non-FHS flag\n",
    "location_filter = all_location_filter\n",
    "\n",
    "\n",
    "year_filter = ('year_id', 'in', year_ids)\n",
    "\n",
    "print(f\"Processing SSP scenario: {ssp_scenario}\")\n",
    "scenario = ssp_scenarios[ssp_scenario][\"dhs_scenario\"]\n",
    "print(f\"Scenario: {scenario}\")\n",
    "\n",
    "def get_file_path(draw, cause, measure, ssp_scenario, dah_scenario=None, vaccinate = None, hold_variable=None):\n",
    "    \"\"\"Generate file path based on cause type\"\"\"\n",
    "    return processed_forecast_ds_path_template.format(\n",
    "        UPLOAD_DATA_PATH=UPLOAD_DATA_PATH,\n",
    "        cause=cause,\n",
    "        measure=measure,\n",
    "        ssp_scenario=ssp_scenario,\n",
    "        dah_scenario=dah_scenario,\n",
    "        vaccinate=vaccinate,\n",
    "        draw=draw,\n",
    "        hold_variable=hold_variable\n",
    "    )\n",
    "\n",
    "# Generate all file paths for all draws\n",
    "file_paths = [get_file_path(draw, cause, measure, ssp_scenario, dah_scenario, vaccinate, hold_variable) \n",
    "              for draw in ssp_draws]\n",
    "\n",
    "print(f\"Loading {len(file_paths)} files...\")\n",
    "\n",
    "# Open all files as a single dataset with lazy loading\n",
    "upload_ds = xr.open_mfdataset(\n",
    "    file_paths,\n",
    "    combine='nested',\n",
    "    concat_dim='draw_id',  # This creates a new dimension for the draws\n",
    "    chunks='auto',  # Enable dask for lazy loading and memory efficiency\n",
    "    drop_variables=['gbd_location_id', 'aa_count', 'level']\n",
    ")\n",
    "\n",
    "# Assign proper draw names to the new dimension\n",
    "upload_ds = upload_ds.assign_coords(draw_id=ssp_draws)\n",
    "\n",
    "# The variables will be named after the draw_dim coordinate values (which are the ssp_draws)\n",
    "# So they should already have the correct names like '000', '001', etc.\n",
    "print(f\"Variable names created: {list(upload_ds.data_vars)}\")\n",
    "\n",
    "print(\"Data loading complete (lazy - data stays on disk until computed)\")\n",
    "print(f\"Dataset variables: {list(upload_ds.data_vars)}\")\n",
    "print(f\"Dataset shape: {upload_ds.dims}\")\n",
    "\n",
    "# ULTRA-FAST ALTERNATIVE: Use xr.zeros_like and reindex\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Get the existing coordinates from your dataset\n",
    "existing_location_ids = upload_ds.coords['location_id'].values\n",
    "year_ids = upload_ds.coords['year_id'].values  \n",
    "age_group_ids = upload_ds.coords['age_group_id'].values\n",
    "sex_ids = upload_ds.coords['sex_id'].values\n",
    "draw_ids = upload_ds.coords['draw_id'].values\n",
    "\n",
    "# Load age metadata\n",
    "age_metadata_df = read_parquet_with_integer_ids(age_metadata_path)\n",
    "age_group_ids_full = age_metadata_df[\"age_group_id\"].unique()\n",
    "\n",
    "# Find missing location IDs\n",
    "sex_ids_full = [1, 2]\n",
    "missing_location_ids = set(all_location_ids) - set(existing_location_ids)\n",
    "missing_location_ids.discard(44858)\n",
    "missing_location_ids = list(missing_location_ids)\n",
    "\n",
    "# Create the complete coordinate space we want\n",
    "complete_coords = {\n",
    "    'location_id': sorted(list(existing_location_ids) + missing_location_ids),\n",
    "    'year_id': year_ids,\n",
    "    'age_group_id': age_group_ids_full,\n",
    "    'sex_id': sex_ids_full,\n",
    "    'draw_id': ssp_draws\n",
    "}\n",
    "\n",
    "# Reindex the original dataset to the complete coordinate space\n",
    "upload_ds_complete = upload_ds.reindex(complete_coords, fill_value=0.0)\n",
    "\n",
    "# Update the dataset reference\n",
    "as_ds = upload_ds_complete\n",
    "as_ds = as_ds.rename({'count_pred': 'val'})\n",
    "as_mean_ds = as_ds.to_array().mean(dim='draw_id')\n",
    "\n",
    "################ Calculate all-age mean and sum ################\n",
    "aa_ds = upload_ds.sum(dim=['sex_id', 'age_group_id'])\n",
    "aa_mean_ds = aa_ds.to_array().mean(dim='draw_id')\n",
    "\n",
    "############### Write the datasets to NetCDF files ################\n",
    "# Write the all-age mean to NetCDF\n",
    "print(f\"Writing to all-age mean to {aa_upload_mean_file_path}\")\n",
    "write_netcdf(\n",
    "    ds=aa_mean_ds,\n",
    "    filepath=aa_upload_mean_file_path,\n",
    "    compression_level=4,  # Good balance for large files\n",
    "    max_chunk_size=2000,  # Larger chunks for forecast data\n",
    "    chunk_threshold=500000  # Lower threshold for better chunking\n",
    ")\n",
    "\n",
    "\n",
    "# Write the all-age draws to NetCDF\n",
    "print(f\"Writing to all-age draws  to {aa_upload_draws_file_path}\")\n",
    "write_netcdf(\n",
    "    ds=aa_ds,\n",
    "    filepath=aa_upload_draws_file_path,\n",
    "    compression_level=4,  # Good balance for large files\n",
    "    max_chunk_size=2000,  # Larger chunks for forecast data\n",
    "    chunk_threshold=500000  # Lower threshold for better chunking\n",
    ")\n",
    "\n",
    "# Write the age-specific mean to NetCDF\n",
    "print(f\"Writing to age-specific mean to {as_upload_mean_file_path}\")\n",
    "write_netcdf(\n",
    "    ds=as_mean_ds,\n",
    "    filepath=as_upload_mean_file_path,\n",
    "    compression_level=4,  # Good balance for large files\n",
    "    max_chunk_size=2000,  # Larger chunks for forecast data\n",
    "    chunk_threshold=500000  # Lower threshold for better chunking\n",
    ")\n",
    "\n",
    "# # Write the age- and sex-specific draws to NetCDF\n",
    "print(f\"Writing to age- and sex-specific draws to {as_upload_draws_file_path}\")\n",
    "write_netcdf(\n",
    "    ds=as_ds,\n",
    "    filepath=as_upload_draws_file_path,\n",
    "    compression_level=1,  # Fastest compression\n",
    "    chunk_by_dim={\n",
    "        'location_id': 1500,  # Chunk locations into groups of 1500\n",
    "        'year_id': 79,        # Keep all years together\n",
    "        'age_group_id': 25,   # Keep all ages together\n",
    "        'sex_id': 2           # Keep both sexes together\n",
    "    },\n",
    "    use_temp_file=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
